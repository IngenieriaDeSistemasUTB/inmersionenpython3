% ch15.tex
% This work is licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 License.
% To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-sa/3.0/nz
% or send a letter to Creative Commons, 171 Second Street, Suite 300, San Francisco, California, 94105, USA.

\chapter{Caso de estudio: portar chardet a Python 3}\label{ch:chardet}

\noindent Nivel de dificultad:\diflllll

\begin{citaCap}
    ``Palabras, palabras. Son todo lo que tenemos para continuar.'' \\
        ---\emph{Rosencrantz y Guildenstern han muerto\footnote{\href{http://www.imdb.com/title/tt0100519/quotes}{http://www.imdb.com/title/tt0100519/quotes}}}
\end{citaCap}

\section{Inmersión}

Pregunta: ¿Cuál es la primera causa de texto ininteligible en la web, en tu buzón de entrada, y en cualquier ordenador? Es la codificación de caracteres. En el capítulo \ref{ch:cadenas}, sobre las cadenas de caracteres, hablé sobre la historia de la codificación de caracteres y la creación de Unicode, la “codificación de caracteres para gobernarlas a todas”. Me gustaría no volver a ver ningún carácter ininteligible en una página web nunca más, porque todos los sistemas de creación de contenidos almacenaran información precisa sobre la codificación de caracteres; todos los protocolos de transferencia de información fueran conscientes de la existencia de Unicode, y todos los sistemas manejaran el texto con fidelidad al convertirlo entre diferentes codificaciones.

También me gustaría un pony.

Un pony de Unicode.

Un Unipony, podría decir.

Creo que me decantaré por la detección automática de la codificación de caracteres.

\section{¿En qué consiste la detección automática?}

Consiste en tomar una secuencia de bytes con una codificación de caracteres desconocida, e intentar determinar cuál es con el fin de poder leer el texto representado. Es como craquear un código cuando no dispones de la clave de decodificación.

\subsection{¿Eso no es imposible?}

En general: sí. Sin embargo, algunas codificaciones están optimizadas para idiomas específicos, y los idiomas no son aleatorios. Algunas secuencias de caracteres se repiten constantemente, mientras que otras no tienen sentido. Una persona con un inglés fluido que abre un períodico y lee “txzqJv 2!dasd0a QqdKjvz” reconoce instantáneamente que eso no es inglés (incluso aunque esté compuesto de letras ingles). Mediante el estudio de grandes cantidades de texto “típico”, un algoritmo puede simular esta clase de “lectura” fluida y proponer la codificación de caracteres en la que puede encontrar un texto.

En otras palabras, la detección de la codificación de caracteres es en realidad la detección del idioma, combinada con el conocimiento de las codificaciones de caracteres que tienden a utilizar.

\subsection{¿Existe tal algoritmo?}

Resulta que sí. Todos los navegadores tienen autodetección de la codificación de caracteres ya que la web está llena de páginas que no tienen ninguna información al respecto. Mozilla Firefox contiene una librería de autodetección de la codificación de caracteres\footnote{\href{http://lxr.mozilla.org/seamonkey/source/extensions/universalchardet/src/base/}{http://lxr.mozilla.org/seamonkey/source/extensions/universalchardet/src/base/}} que es de código abierto. Yo porté dicha librería a Python 2 bajo el módulo denominado \codigo{chardet}. Este capítulo describe los pasos a través del proceso de conversión del módulo \codigo{charffdet} de Python 2 a Python 3.

\section{Introducción al módulo \codigo{chardet}}

\cajaTexto{La detección de la codificación de caracteres es en realidad la detección del lenguaje con dificultades.}

Antes de convertir el código, ayudaría que entendieras cómo funciona. Esta es una guía breve a través del propio código. La librería \codigo{chardet} es demasiado larga para incluirla completa aquí, pero puedes descargarla de \codigo{chardet.feedparser.org}\footnote{\href{http://chardet.feedparser.org/download/}{http://chardet.feedparser.org/download/}}.


El punto de entrada principal para el algoritmo de detección es \codigo{universaldetector.py}, que contiene una clase, \codigo{UniversalDetector} \footnote{Podrías pensar que el punto de entrada principal es la función \codigo{detect} en \codigo{chardet/\_\_init\_\_.py}, pero en realidad esto es una función de conveniencia para crear un objeto \codigo{UniversalDetector} , llamarlo, y devolver su resultado}.

\codigo{UniversalDetector} puede manejar cinco categorías de codificaciones de caracteres:

\begin{enumerate}
  \item \codigo{UTF-N} con \codigo{BOM}\footnote{Ver capítulo \ref{ch:cadenas} sobre Cadenas de Caracteres}, con las variantes ``Big-Endian'' y ``Little-Endian'' de \codigo{UTF-16}, y todas las variantes de 4-bytes de \codigo{UTF-16}.

  \item Codificaciones de escape, compatibles con \codigo{ASCII} de 7-bits, 
    en las que los caracteres \codigo{no-ASCII} comienzan con una secuencia de escape. Por ejemplo: \codigo{ISO-2022-JP} (japonés) y \codigo{HZ-GB-2312}(chino).

  \item Codificaciones multibyte, en las que cada carácter se representa por un número variable de bytes. Por ejemplo: \codigo{BIG5} (chino), \codigo{SHIFT\_JIS} (japonés), \codigo{EUC-KR} (coreano), y \codigo{UTF-8} sin \codigo{BOM}.

  \item Codificaciones de un solo byte, en las que cada carácter se representa por un único byte. Por ejemplo: \codigo{KOI8-R} (ruso), \codigo{WINDOWS-1255} (hebreo), y \codigo{TIS-620} (thai).

  \item \codigo{WINDOWS-1252}, que se utiliza fundamentalmente en Microsoft Windows por los mandos intermedios que no distinguen una codificación de caracteres de un agujero en el suelo.

\end{enumerate}

\subsection{\codigo{UTF-N} con \codigo{BOM}}

Si el texto comienza con una marca \codigo{BOM}, se puede asumir de forma razonable que está codificado en \codigo{UTF-8}, \codigo{UTF-16} o \codigo{UTF-32} (Precisamente el \codigo{BOM} sirve para indicar cuál de ellos es). Esto es manejado por la propia clase \codigo{UniversalDetector}, que retorna el resultado inmediatamente sin proceso adicional.

\subsection{Codificaciones con código de escape}

Si el texto contiene una cadena de caracteres de escape reconocible podría indicar que se encuentra en una de las codificaciones que se basan en ello. \codigo{UniversalDetector} crea un objeto \codigo{EscCharSetProber} (definido en el \codigo{escprober.py}) y le pasa el texto.

\codigo{ExcCharSetProber} crea una serie de máquinas de estado, basadas en los modelos definidos en \codigo{escsm.py}): \codigo{HZ-GB-2312}, \codigo{ISO-2022-CN}, \codigo{ISO-2022-JP}, y \codigo{ISO-2022-KR}. \codigo{EscCharSetProber} alimenta el texto a cada una de las máquinas de estado, byte a byte. Si alguna de ellas finaliza identificando la codificación, \codigo{EscCharSetProber} finaliza devolviendo el resultado a \codigo{UniversalDetector}, que lo retorna al llamante. Si cualquiera de las máquinas de estado alcanza una secuencia ilegal, finaliza su ejecución y se sigue la ejecución con la siguiente máquina de estados.

\subsection{Codificaciones multibyte}

Asumiendo que no existe \codigo{BOM}, \codigo{UniversalDetector} chequea si el texto contiene algún carácter con bits altos activados. Si es así, crea una serie de ``sondas'' para detectar codificaciones multibyte, de un byte y, como último recurso, \codigo{windos-1252}.

La sonda de codificaciones multibyte, \codigo{MBCSGroupProber} (definida en \codigo{mbcsgroupprober.py}), es en realidad un envoltorio que gestiona un grupo de sondas, una para cada tipo de codificación multibyte: \codigo{BIG5}, \codigo{GB2313}, \codigo{EUC-TW}, \codigo{EUC-KR}, \codigo{EUC-JP}, \codigo{SHIFT\_JIS}, y \codigo{UTF-8}. \codigo{MBCSGroupProber} alimenta el texto a cada una de las sondas específicas y chequea el resultado. Si una sonda reporta una secuencia de bytes ilegal, se elimina de la búsqueda (cualquier llamada posterior a \codigo{UniversalDetector.feed()} para este texto se saltará a esta sonda). Si una sonda informa que está segura razonablemente de que ha detectado la codificación, \codigo{MBCSGroupProber} informa del resultado positivo a \codigo{UniversalDetector}, que devuelve el resultado al llamante.

La mayoría de las sondas de la codificación multibyte heredan de \codigo{MultiByteCharSetProber} (definida en \codigo{mbcharsetprober.py}, y simplemente activan la máquina de estados y analizador de distribución apropiado y dejan a la clase \codigo{MultiByteCharSetProber} hacer el resto del trabajo. \codigo{MultiByteCharSetProber} recorre el texto a traves de la máquina de estados específica byte a byte, para buscar secuencias de caracteres que pudieran indicar de forma concluyente un resultado positivo o negativo. Al mismo tiempo, \codigo{MultiByteCharSetProber} alimenta el texto a un analizador de distribución específico de cada codificación de caracteres.

Los analizadores de distribución (definidos en \codigo{chardistribution.py}) utilizan modelos específicos para cada idioma que tienen en cuenta los caracteres más frecuentes en cada uno de ellos. Cuando \codigo{MultiByteCharSetProber} ha alimentado suficiente texto a los analizadores, calcula el grado de confianza basándose en el número de caracteres más frecuentes, el número total de caracteres, y el ratio de distribución específico de cada lenguaje. Si el grado de confianza es suficientemente algo, \codigo{MultiByteCharSetProber} devuelve el resultado a \codigo{MBCSGroupProber}, que lo devuelve a \codigo{UniversalDetector}, quien, a su vez, lo devuelve al llamante.

El caso del idioma Japonés es más difícil. Un análisis de distribución monocarácter no es siempre suficiente para distinguir entre \codigo{EUC-JP} y \codigo{SHIFT\_JIS}, por ello la sonda \codigo{SJISProber} (definida en \codigo{sjisprober.py} también utiliza un análisis de distribución de dos caracteres. \codigo{SJISContextAnalysis} y \codigo{EUCJPContextAnalysis} (ambos definidos en \codigo{jpcntx.py}) comprueban la frecuencia en el texto de los caracteres del silabario Hiragana. Cuando se ha procesado texto suficiente, devuelven el grado de confianza a \codigo{SJISProber}, que chequea ambos analizadores y devuelve aquél de mayor grado de confianza a \codigo{MBCSGroupProber}.

