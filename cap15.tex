% ch15.tex
% This work is licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 License.
% To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-sa/3.0/nz
% or send a letter to Creative Commons, 171 Second Street, Suite 300, San Francisco, California, 94105, USA.

\chapter{Caso de estudio: portar chardet a Python 3}\label{ch:chardet}

\noindent Nivel de dificultad:\diflllll

\begin{citaCap}
    ``Palabras, palabras. Son todo lo que tenemos para continuar.'' \\
        ---\emph{Rosencrantz y Guildenstern han muerto\footnote{\href{http://www.imdb.com/title/tt0100519/quotes}{http://www.imdb.com/title/tt0100519/quotes}}}
\end{citaCap}

\section{Inmersión}

Pregunta: ¿Cuál es la primera causa de texto ininteligible en la web, en tu buzón de entrada, y en cualquier ordenador? Es la codificación de caracteres. En el capítulo \ref{ch:cadenas}, sobre las cadenas de caracteres, hablé sobre la historia de la codificación de caracteres y la creación de Unicode, la “codificación de caracteres para gobernarlas a todas”. Me gustaría no volver a ver ningún carácter ininteligible en una página web nunca más, porque todos los sistemas de creación de contenidos almacenaran información precisa sobre la codificación de caracteres; todos los protocolos de transferencia de información fueran conscientes de la existencia de Unicode, y todos los sistemas manejaran el texto con fidelidad al convertirlo entre diferentes codificaciones.

También me gustaría un pony.

Un pony de Unicode.

Un Unipony, podría decir.

Creo que me decantaré por la detección automática de la codificación de caracteres.

\section{¿En qué consiste la detección automática?}

Consiste en tomar una secuencia de bytes con una codificación de caracteres desconocida, e intentar determinar cuál es con el fin de poder leer el texto representado. Es como craquear un código cuando no dispones de la clave de decodificación.

\subsection{¿Eso no es imposible?}

En general: sí. Sin embargo, algunas codificaciones están optimizadas para idiomas específicos, y los idiomas no son aleatorios. Algunas secuencias de caracteres se repiten constantemente, mientras que otras no tienen sentido. Una persona con un inglés fluido que abre un períodico y lee “txzqJv 2!dasd0a QqdKjvz” reconoce instantáneamente que eso no es inglés (incluso aunque esté compuesto de letras ingles). Mediante el estudio de grandes cantidades de texto “típico”, un algoritmo puede simular esta clase de “lectura” fluida y proponer la codificación de caracteres en la que puede encontrar un texto.

En otras palabras, la detección de la codificación de caracteres es en realidad la detección del idioma, combinada con el conocimiento de las codificaciones de caracteres que tienden a utilizar.

\subsection{¿Existe tal algoritmo?}

Resulta que sí. Todos los navegadores tienen autodetección de la codificación de caracteres ya que la web está llena de páginas que no tienen ninguna información al respecto. Mozilla Firefox contiene una librería de autodetección de la codificación de caracteres\footnote{\href{http://lxr.mozilla.org/seamonkey/source/extensions/universalchardet/src/base/}{http://lxr.mozilla.org/seamonkey/source/extensions/universalchardet/src/base/}} que es de código abierto. Yo porté dicha librería a Python 2 bajo el módulo denominado \codigo{chardet}. Este capítulo describe los pasos a través del proceso de conversión del módulo \codigo{charffdet} de Python 2 a Python 3.

\section{Introducción al módulo \codigo{chardet}}

\cajaTexto{La detección de la codificación de caracteres es en realidad la detección del lenguaje con dificultades.}

Antes de convertir el código, ayudaría que entendieras cómo funciona. Esta es una guía breve a través del propio código. La librería \codigo{chardet} es demasiado larga para incluirla completa aquí, pero puedes descargarla de \codigo{chardet.feedparser.org}\footnote{\href{http://chardet.feedparser.org/download/}{http://chardet.feedparser.org/download/}}.


El punto de entrada principal para el algoritmo de detección es \codigo{universaldetector.py}, que contiene una clase, \codigo{UniversalDetector} \footnote{Podrías pensar que el punto de entrada principal es la función \codigo{detect} en \codigo{chardet/\_\_init\_\_.py}, pero en realidad esto es una función de conveniencia para crear un objeto \codigo{UniversalDetector} , llamarlo, y devolver su resultado}.

\codigo{UniversalDetector} puede manejar cinco categorías de codificaciones de caracteres:

\begin{enumerate}
  \item \codigo{UTF-N} con \codigo{BOM}\footnote{Ver capítulo \ref{ch:cadenas} sobre Cadenas de Caracteres}, con las variantes ``Big-Endian'' y ``Little-Endian'' de \codigo{UTF-16}, y todas las variantes de 4-bytes de \codigo{UTF-16}.

  \item Codificaciones de escape, compatibles con \codigo{ASCII} de 7-bits, 
    en las que los caracteres \codigo{no-ASCII} comienzan con una secuencia de escape. Por ejemplo: \codigo{ISO-2022-JP} (japonés) y \codigo{HZ-GB-2312}(chino).

  \item Codificaciones multibyte, en las que cada carácter se representa por un número variable de bytes. Por ejemplo: \codigo{BIG5} (chino), \codigo{SHIFT\_JIS} (japonés), \codigo{EUC-KR} (coreano), y \codigo{UTF-8} sin \codigo{BOM}.

  \item Codificaciones de un solo byte, en las que cada carácter se representa por un único byte. Por ejemplo: \codigo{KOI8-R} (ruso), \codigo{WINDOWS-1255} (hebreo), y \codigo{TIS-620} (thai).

  \item \codigo{WINDOWS-1252}, que se utiliza fundamentalmente en Microsoft Windows por los mandos intermedios que no distinguen una codificación de caracteres de un agujero en el suelo.

\end{enumerate}

\subsection{\codigo{UTF-N} con \codigo{BOM}}

Si el texto comienza con una marca \codigo{BOM}, se puede asumir de forma razonable que está codificado en \codigo{UTF-8}, \codigo{UTF-16} o \codigo{UTF-32} (Precisamente el \codigo{BOM} sirve para indicar cuál de ellos es). Esto es manejado por la propia clase \codigo{UniversalDetector}, que retorna el resultado inmediatamente sin proceso adicional.

\subsection{Codificaciones con código de escape}

Si el texto contiene una cadena de caracteres de escape reconocible podría indicar que se encuentra en una de las codificaciones que se basan en ello. \codigo{UniversalDetector} crea un objeto \codigo{EscCharSetProber} (definido en el \codigo{escprober.py}) y le pasa el texto.

\codigo{ExcCharSetProber} crea una serie de máquinas de estado, basadas en los modelos definidos en \codigo{escsm.py}): \codigo{HZ-GB-2312}, \codigo{ISO-2022-CN}, \codigo{ISO-2022-JP}, y \codigo{ISO-2022-KR}. \codigo{EscCharSetProber} alimenta el texto a cada una de las máquinas de estado, byte a byte. Si alguna de ellas finaliza identificando la codificación, \codigo{EscCharSetProber} finaliza devolviendo el resultado a \codigo{UniversalDetector}, que lo retorna al llamante. Si cualquiera de las máquinas de estado alcanza una secuencia ilegal, finaliza su ejecución y se sigue la ejecución con la siguiente máquina de estados.

\subsection{Codificaciones multibyte}

Asumiendo que no existe \codigo{BOM}, \codigo{UniversalDetector} chequea si el texto contiene algún carácter con bits altos activados. Si es así, crea una serie de ``sondas'' para detectar codificaciones multibyte, de un byte y, como último recurso, \codigo{windos-1252}.

La sonda de codificaciones multibyte, \codigo{MBCSGroupProber} (definida en \codigo{mbcsgroupprober.py}), es en realidad un envoltorio que gestiona un grupo de sondas, una para cada tipo de codificación multibyte: \codigo{BIG5}, \codigo{GB2313}, \codigo{EUC-TW}, \codigo{EUC-KR}, \codigo{EUC-JP}, \codigo{SHIFT\_JIS}, y \codigo{UTF-8}. \codigo{MBCSGroupProber} alimenta el texto a cada una de las sondas específicas y chequea el resultado. Si una sonda reporta una secuencia de bytes ilegal, se elimina de la búsqueda (cualquier llamada posterior a \codigo{UniversalDetector.feed()} para este texto se saltará a esta sonda). Si una sonda informa que está segura razonablemente de que ha detectado la codificación, \codigo{MBCSGroupProber} informa del resultado positivo a \codigo{UniversalDetector}, que devuelve el resultado al llamante.

La mayoría de las sondas de la codificación multibyte heredan de \codigo{MultiByteCharSetProber} (definida en \codigo{mbcharsetprober.py}, y simplemente activan la máquina de estados y analizador de distribución apropiado y dejan a la clase \codigo{MultiByteCharSetProber} hacer el resto del trabajo. \codigo{MultiByteCharSetProber} recorre el texto a traves de la máquina de estados específica byte a byte, para buscar secuencias de caracteres que pudieran indicar de forma concluyente un resultado positivo o negativo. Al mismo tiempo, \codigo{MultiByteCharSetProber} alimenta el texto a un analizador de distribución específico de cada codificación de caracteres.

Los analizadores de distribución (definidos en \codigo{chardistribution.py}) utilizan modelos específicos para cada idioma que tienen en cuenta los caracteres más frecuentes en cada uno de ellos. Cuando \codigo{MultiByteCharSetProber} ha alimentado suficiente texto a los analizadores, calcula el grado de confianza basándose en el número de caracteres más frecuentes, el número total de caracteres, y el ratio de distribución específico de cada lenguaje. Si el grado de confianza es suficientemente algo, \codigo{MultiByteCharSetProber} devuelve el resultado a \codigo{MBCSGroupProber}, que lo devuelve a \codigo{UniversalDetector}, quien, a su vez, lo devuelve al llamante.

El caso del idioma Japonés es más difícil. Un análisis de distribución monocarácter no es siempre suficiente para distinguir entre \codigo{EUC-JP} y \codigo{SHIFT\_JIS}, por ello la sonda \codigo{SJISProber} (definida en \codigo{sjisprober.py} también utiliza un análisis de distribución de dos caracteres. \codigo{SJISContextAnalysis} y \codigo{EUCJPContextAnalysis} (ambos definidos en \codigo{jpcntx.py}) comprueban la frecuencia en el texto de los caracteres del silabario Hiragana. Cuando se ha procesado texto suficiente, devuelven el grado de confianza a \codigo{SJISProber}, que chequea ambos analizadores y devuelve aquél de mayor grado de confianza a \codigo{MBCSGroupProber}.

\subsection{Codificaciones de un solo byte}

\cajaTexto{En serio, dónde está mi pony unicode}

La sonda de codificaciones de un solo byte, \codigo{SBCSGroupProber} (definida en \codigo{sbcsgroupprober.py}), también es un envoltorio que gestiona el grupo de sondas que existen para cada combinación de idioma y codificación de un solo byte: \codigo{windows-1251}, \codigo{KOI8-R}, \codigo{ISO-8859-5}, \codigo{MacCyrillic}, \codigo{IBM855}, y \codigo{IBM866} (ruso); \codigo{ISO-8859-7} y \codigo{windows-1253} (griego); \codigo{ISO-8859-5} y \codigo{windows-1251} (búlgaro); \codigo{ISO-8859-2} y \codigo{windows-1250} (húngaro); \codigo{TIS-620} (Thai); \codigo{windows-1255} e \codigo{ISO-8859-8} (Hebreo).

\codigo{SBCSGroupProber} alimenta el texto a cada uno de estas sondas específicas y comprueba sus resultados. Las sondas están implementadas con un única clase \codigo{SingleByteCharSetProber} (definida en \codigo{sbcharsetprober.py}, que utiliza como parámetro del constructor el modelo del idioma. Este define la frecuencia de aparición de las diferentes secuencias de dos caracteres en un texto típico. \codigo{SingleByteByteCharSetProber} procesa el texto contando las secuencias de dos caracteres más frecuentes. Cuando se ha procesado suficiente texto, calcula el nivel de confianza basado en el número de dichas secuencias, el número total de caracteres, y la distribución específica del idioma.

El hebreo se maneja como un caso especial. Si el texto parece hebreo, basado en el análisis de distribución de dos caracteres, \codigo{HebrewProber} (definido en \codigo{hebrewprober.py} intenta distinguir entre hebreo visual (en el que el texto está realmente almacenado ``hacia atrás'' línea a línea, y luego se muestra directamente para que pueda leerse de derecha a izquierda) y hebreo lógico (en el que el texto se almacena en el orden de lectura y luego se visualiza de derecha a izquierda por el sistema). Debido a que ciertos caracteres se codifican de forma diferente en función de que aparezcan en medio de una palabra o al final, podemos intentar adivinar de forma razonable la dirección del texto fuente, y devolver la codificación correcta (\codigo{windows-1255} en el caso de hebreo lógico, o \codigo{ISO-8859-8} para el hebreo visual).

\subsection{windows-1252}

Si \codigo{UniversalDetector} encuentra en el texto un carácter con el bit alto activado, pero ninguno de las sondas anteriores devuelve un resultado fiable, crea un objeto \codigo{Latin1Prober} (definido en \codigo{latin1prober.py}) para intentar detectar texto en inglés en una codificación \codigo{windows-1252}. Esta no es fiable (inherentemente), porque las letras en inglés se codifican de la misma forma en diferentes codificaciones. La única forma de distinguir \codigo{windows-1252} es a través de algunos símbolos comúnmente utilizados como las comillas inteligentes, los apostrofos, símbolos de copyright, y otros similares. \codigo{Latin1Prober} reduce su estimación del nivel de confianza para permitir que le ``ganen'' otras sondas más precisas, si es posible.

\section{Ejecutando \codigo{2to3}}

Vamos a migrar el módulo \codigo{chardet} de Python 2 a Python 3. Este último trae una utilidad denominada \codigo{2to3}, que toma el código fuente de Python 2 como entrada y lo convierte de forma automática a Python 3. En algunos casos es fácil ---Cambiar una función de librería que fue renombrada o movida a otro módulo---, pero en otros casos puede ser muy complejo. Para entender lo que puede llegar a hacer, lee el apéndice \ref{ap:porting}, Migrando código a Python 3 con \codigo{2to3}. En este capítulo comenzaremos ejecutando \codigo{2to3} en el paquete \codigo{chardet}, verás que aún quedará bastante trabajo que hacer después de que las herramientas automatizadas hayan aplicado su magia.

El paquete \codigo{chardet} está dividido en varios ficheros, todos en el mismo directorio. El script \codigo{2to3} facilita la conversión de varios ficheros a la vez: basta con pasarle el nombre del directorio como parámetro y \codigo{2to3} convertirá cada uno de los ficheros que contenga.


\begin{lstlisting}[escapeinside={(*}{*)}]
C:\home\chardet> python c:\Python30\Tools\Scripts\2to3.py -w chardet\
RefactoringTool: Skipping implicit fixer: buffer
RefactoringTool: Skipping implicit fixer: idioms
RefactoringTool: Skipping implicit fixer: set_literal
RefactoringTool: Skipping implicit fixer: ws_comma
--- chardet\__init__.py (original)
+++ chardet\__init__.py (refactored)
@@ -18,7 +18,7 @@
 __version__ = "1.0.1"

 def detect(aBuf):
(* \color{red}-    import universaldetector *)
(* \color{green!50!black}+    from . import universaldetector *)
     u = universaldetector.UniversalDetector()
     u.reset()
     u.feed(aBuf)
--- chardet\big5prober.py (original)
+++ chardet\textbackslash big5prober.py (refactored)
@@ -25,10 +25,10 @@
 # 02110-1301  USA
 ######################### END LICENSE BLOCK #########################

(* \color{red}-from mbcharsetprober import MultiByteCharSetProber *)
(* \color{red}-from codingstatemachine import CodingStateMachine *)
(* \color{red}-from chardistribution import Big5DistributionAnalysis *)
(* \color{red}-from mbcssm import Big5SMModel *)
(* \color{green!50!black}+from .mbcharsetprober import MultiByteCharSetProber *)
(* \color{green!50!black}+from .codingstatemachine import CodingStateMachine *)
(* \color{green!50!black}+from .chardistribution import Big5DistributionAnalysis *)
(* \color{green!50!black}+from .mbcssm import Big5SMModel *)

 class Big5Prober(MultiByteCharSetProber):
     def __init__(self):
--- chardet\chardistribution.py (original)
+++ chardet\chardistribution.py (refactored)
@@ -25,12 +25,12 @@
 # 02110-1301  USA
 ######################### END LICENSE BLOCK #########################

(* \color{red}-import constants *)
(* \color{red}-from euctwfreq import EUCTWCharToFreqOrder,  *)
(* \color{red}      EUCTW\_TABLE\_SIZE, EUCTW\_TYPICAL\_DISTRIBUTION\_RATIO *)
(* \color{red}-from euckrfreq import EUCKRCharToFreqOrder, *)
(* \color{red}      EUCKR\_TABLE\_SIZE, EUCKR\_TYPICAL\_DISTRIBUTION\_RATIO *)
(* \color{red}-from gb2312freq import GB2312CharToFreqOrder, *)
(* \color{red}      GB2312\_TABLE\_SIZE, GB2312\_TYPICAL\_DISTRIBUTION\_RATIO *)
(* \color{red}-from big5freq import Big5CharToFreqOrder, *)
(* \color{red}      BIG5\_TABLE\_SIZE, BIG5\_TYPICAL\_DISTRIBUTION\_RATIO *)
(* \color{red}-from jisfreq import JISCharToFreqOrder, *)
(* \color{red}      JIS\_TABLE\_SIZE, JIS\_TYPICAL\_DISTRIBUTION\_RATIO *)
(* \color{green!50!black}+from . import constants *)
(* \color{green!50!black}+from .euctwfreq import EUCTWCharToFreqOrder, *)
(* \color{green!50!black}      EUCTW\_TABLE\_SIZE, EUCTW\_TYPICAL\_DISTRIBUTION\_RATIO *)
(* \color{green!50!black}+from .euckrfreq import EUCKRCharToFreqOrder, *)
(* \color{green!50!black}      EUCKR\_TABLE\_SIZE, EUCKR\_TYPICAL\_DISTRIBUTION\_RATIO *)
(* \color{green!50!black}+from .gb2312freq import GB2312CharToFreqOrder, *)
(* \color{green!50!black}      GB2312\_TABLE\_SIZE, GB2312\_TYPICAL\_DISTRIBUTION\_RATIO *)
(* \color{green!50!black}+from .big5freq import Big5CharToFreqOrder,  *)
(* \color{green!50!black}      BIG5\_TABLE\_SIZE, BIG5\_TYPICAL\_DISTRIBUTION\_RATIO *)
(* \color{green!50!black}+from .jisfreq import JISCharToFreqOrder,  *)
(* \color{green!50!black}      JIS\_TABLE\_SIZE, JIS\_TYPICAL\_DISTRIBUTION\_RATIO *)

 ENOUGH_DATA_THRESHOLD = 1024
 SURE_YES = 0.99
.
.
(* \color{yellow!75!black}. (Durante un rato va sacando mensajes de este tipo) *)
.
.
RefactoringTool: Files that were modified:
RefactoringTool: chardet\__init__.py
RefactoringTool: chardet\big5prober.py
RefactoringTool: chardet\chardistribution.py
RefactoringTool: chardet\charsetgroupprober.py
RefactoringTool: chardet\codingstatemachine.py
RefactoringTool: chardet\constants.py
RefactoringTool: chardet\escprober.py
RefactoringTool: chardet\escsm.py
RefactoringTool: chardet\eucjpprober.py
RefactoringTool: chardet\euckrprober.py
RefactoringTool: chardet\euctwprober.py
RefactoringTool: chardet\gb2312prober.py
RefactoringTool: chardet\hebrewprober.py
RefactoringTool: chardet\jpcntx.py
RefactoringTool: chardet\langbulgarianmodel.py
RefactoringTool: chardet\langcyrillicmodel.py
RefactoringTool: chardet\langgreekmodel.py
RefactoringTool: chardet\langhebrewmodel.py
RefactoringTool: chardet\langhungarianmodel.py
RefactoringTool: chardet\langthaimodel.py
RefactoringTool: chardet\latin1prober.py
RefactoringTool: chardet\mbcharsetprober.py
RefactoringTool: chardet\mbcsgroupprober.py
RefactoringTool: chardet\mbcssm.py
RefactoringTool: chardet\sbcharsetprober.py
RefactoringTool: chardet\sbcsgroupprober.py
RefactoringTool: chardet\sjisprober.py
RefactoringTool: chardet\universaldetector.py
RefactoringTool: chardet\utf8prober.py

Now run the 2to3 script on the testing harness, test.py.

C:\home\chardet> python c:\Python30\Tools\Scripts\2to3.py -w test.py
RefactoringTool: Skipping implicit fixer: buffer
RefactoringTool: Skipping implicit fixer: idioms
RefactoringTool: Skipping implicit fixer: set_literal
RefactoringTool: Skipping implicit fixer: ws_comma
--- test.py (original)
+++ test.py (refactored)
@@ -4,7 +4,7 @@
 count = 0
 u = UniversalDetector()
 for f in glob.glob(sys.argv[1]):
(* \color{red}-    print f.ljust(60), *)
(* \color{green!50!black}+    print(f.ljust(60), end=' ') *)
     u.reset()
     for line in file(f, 'rb'):
         u.feed(line)
@@ -12,8 +12,8 @@
     u.close()
     result = u.result
     if result['encoding']:
(* \color{red}-        print result['encoding'], 'with confidence', result['confidence'] *)
(* \color{green!50!black}+        print(result['encoding'], 'with confidence', result['confidence']) *)
     else:
(* \color{red}-        print '******** no result' *)
(* \color{green!50!black}+        print('******** no result') *)
     count += 1
(* \color{red}-print count, 'tests' *)
(* \color{green!50!black}+print(count, 'tests') *)
RefactoringTool: Files that were modified:
RefactoringTool: test.py
\end{lstlisting}

Bueno, no fue para tanto. Solo se han convertido unos pocos imports y sentencias print. Por cierto, cuál era el problema con todas las sentencias import. Para contestar a esto, tienes que entender que el módulo \codigo{chardet} estaba dividido en múltiples ficheros.

\section{Una breve disgresión sobre los módulos \emph{multifichero}}

\codigo{chardet} es un módulo multifichero. Podría haber elegido poner todo el código en uno solo (denominado \codigo{chardet.py}, pero no lo hice. En vez de eso, creé un directorio (denominado \codigo{chardet}), luego creé un fichero \codigo{\_\_init\_\_.py} en él, \emph{con ello se asume que todos los ficheros de este directorio son parte del mismo módulo}. El nombre del módulo es el nombre del directorio. Los ficheros que están dentro del directorio pueden referenciar a otros ficheros dentro del mismo, o incluso en subdirectorios (más sobre esto en un minuto). Pero la colección completa de ficheros se presenta para otro código de Python como un único módulo ---como si las funciones y las clases se encontrasen un único fichero de extensión \codigo{.py}.

¿Qué contiene el fichero \codigo{\_\_init\_\_.py}? Nada. Todo. Algo intermedio. El fichero \codigo{\_\_init\_\_.py} no necesita definir nada. Puede ser un fichero vacío. Pero también se puede utilizar para definir en él las funciones que sean punto de entrada a tu módulo. O puedes poner todas las funciones en él. O todas, salvo una\ldots

\cajaTextoAncho{Un directorio con un fichero \codigo{\_\_init\_\_.py} siempre se trata como un módulo multifichero. Sin un fichero \codigo{\_\_init\_\_.py}, un directorio no contiene más que un conjunto de ficheros \codigo{.py} sin relación alguna}

Veamos como funciona esto en la práctica.

\begin{lstlisting}[breaklines=true]
>>> import chardet
>>> dir(chardet)
['__builtins__', '__doc__', '__file__', '__name__',
 '__package__', '__path__', '__version__', 'detect']
>>> chardet
<module 'chardet' from 'C:\Python31\lib\site-packages\chardet\__init__.py'>
\end{lstlisting}

\begin{enumerate}
  \item \emph{Línea 2:} aparte de los atributos de clase habituales, lo único que hay en el módulo \codigo{chardet} es la función \codigo{detect()}.
  \item \emph{Línea 5:} aquí aparece la primera pista de que el módulo \codigo{chardet} está formado por más de un fichero; el ``módulo'' se muestra como procedente del fichero \codigo{\_\_init\_\_.py} del directorio \codigo{chardet/}.
\end{enumerate}

Veamos el contenido del fichero \codigo{\_\_init\_\_.py}.


\begin{lstlisting}[language=Python,breaklines=true]
def detect(aBuf):                              
    from . import universaldetector            
    u = universaldetector.UniversalDetector()
    u.reset()
    u.feed(aBuf)
    u.close()
    return u.result
\end{lstlisting}

\begin{enumerate}
  \item \emph{Línea 1:} El fichero define la función \codigo{detect()}, que es el punto de entrada principal del módulo \codigo{chardet}.
  \item \emph{Línea 2:} Esta función tiene poquísimo código. En realidad, todo lo que hace es importar el módulo \codigo{universaldetector} y comenzar a usarlo. ¿Pero dónde está definido \codigo{universaldetector}?
\end{enumerate}

La respuesta se encuentra en esa extraña sentencia \codigo{import}.

\begin{lstlisting}[language=Python,breaklines=true]
from . import universaldetector
\end{lstlisting}

Traducido, significa que ``se importe el módulo \codigo{universaldetector}; que está en el mismo directorio en el que estoy yo (el fichero \codigo{chardet/\_\_init\_\_.py})''. A esto se le denomina \emph{importación relativa}. Es la forma en que se localizan entre sí los ficheros que se encuentran en un módulo multifichero, sin preocuparse de conflictos de denominación con otros módulos que puedas haber instalado en tu \emph{camino de búsqueda de módulos}\footnote{Ver la sección \ref{sc:search_path}, sobre este tema.}. Esta sentencia \codigo{import} solamente buscará a \codigo{universaldetector} dentro del propio directorio \codigo{chardet}.

Estos dos conceptos ---\codigo{\_\_init\_\_.py} e importación relativa--- permiten que puedas dividir un módulo en las piezas que quieras. El módulo \codigo{chardet} consta de 36 ficheros \codigo{.py}, ¡36!. Aún así, para utilizarlo únicamente necesitas usar \codigo{import chardet}, y luego llamar a la función \codigo{chardet.detect()}. Para tu código es transparente dónde está definida la función, si utiliza una importación relativa de \codigo{universaldetector} y que este, a su vez, utiliza cinco importaciones relativas de otros tantos ficheros, todos contenidos en el directorio \codigo{chardet/}.

\cajaTextoAncho{Si alguna vez te encuentras en la necesidad de escribir una librería grande en Python (o más problemente, cuando te des cuenta de que tu pequeña librería ha crecido hasta convertirse en grande), tómate tu tiempo en refactorizarla en un módulo multifichero. Es una de las muchas cosas en las que Python es muy bueno, así que aprovéchate de ello.}

\section{Arreglando lo que \codigo{2to3} no puede}

\subsection{\codigo{False} es sintaxis inválida}

\cajaTexto{¿Tienes pruebas? ¿No?}

Ahora vamos a las pruebas reales: ejecutar la suite de pruebas. Puesto que la suite de pruebas esta diseñada para cubrir todos los caminos posibles del código, es una buena manera de probar el código migrado para estar seguro de que no hay errores acechando en algún rincón.

\noindent\begin{minipage}{\textwidth}
\begin{lstlisting}[breaklines=true]
C:\home\chardet> python test.py tests\*\*
Traceback (most recent call last):
  File "test.py", line 1, in <module>
    from chardet.universaldetector import UniversalDetector
  File "C:\home\chardet\chardet\universaldetector.py", line 51
    self.done = constants.False
                              ^
SyntaxError: invalid syntax
\end{lstlisting}
\end{minipage}

Vaya, un pequeño fallo. En Python 3, \codigo{False} es una palabra reservada, así que no la puedes utilizar como nombre de una variable. Vamos a mirar \codigo{constants.py} para ver dónde está definida. Aquí está la versión original del fichero antes de que \codigo{2to3} lo cambiara:


\begin{lstlisting}[language=Python,breaklines=true]
import __builtin__
if not hasattr(__builtin__, 'False'):
    False = 0
    True = 1
else:
    False = __builtin__.False
    True = __builtin__.True
\end{lstlisting}

Este código está diseñado para que funcione con versiones antiguas de \mbox{Python 2}. Antes de Python 2.3 no existía el tipo \codigo{bool}. El código detecta la ausencia de las constantes \codigo{True} y \codigo{False} y las define si es necesario.

Sin embargo, en Python 3 siempre existe el tipo \codigo{bool}, por lo que este código es innecesario. La solución más simple pasa por sustituir todas las instancias de \codigo{constants.True} y \codigo{constants.False} por \codigo{True} y \codigo{False}, respectivamente. Y borrar este fichero.

Así, esta línea del fichero \codigo{universaldetector.py}:

\begin{lstlisting}[language=Python,breaklines=true]
self.done = constants.False
\end{lstlisting}

Se convierte en:

\begin{lstlisting}[language=Python,breaklines=true]
self.done = False
\end{lstlisting}

¡Ah! ¿No es satisfactorio? El código queda más corto y más legible así.

\subsection{No hay ningún módulo denominado \codigo{constants}}

Hora de volver a ejecutar \codigo{test.py} y ver hasta donde llega.

\noindent\begin{minipage}{\textwidth}
\begin{lstlisting}[breaklines=true]
C:\home\chardet> python test.py tests\*\*
Traceback (most recent call last):
  File "test.py", line 1, in <module>
    from chardet.universaldetector import UniversalDetector
  File "C:\home\chardet\chardet\universaldetector.py", line 29, in <module>
    import constants, sys
ImportError: No module named constants
\end{lstlisting}
\end{minipage}

¿Qué dices? ¿Que no hay un módulo denominado \codigo{constants}? Desde luego que sí. Está ahí mismo, en \codigo{chardet/constants.py}.

¿Recuerdas que el comando \codigo{2to3} arregló muchas sentencias \codigo{import}? Esta librería tiene muchas importaciones relativas ---esto es, módulos que importan a otros módulos dentro de la misma librería--- pero la lógica que gobierna las mismas ha cambiado en Python 3. En Python 2, podías escribir \codigo{import constants} y el primer lugar en el que se busca es en el directorio \codigo{chardet/}. En Python 3, todas las sentencias \codigo{import} son absolutas por defecto. Si quieres una relativa hay que ser explícito:

\begin{lstlisting}[language=Python,breaklines=true]
from . import constants
\end{lstlisting}

Pero espera, ¿No se supone que \codigo{2to3} tenía que haber resuelto esto por ti? Bueno, lo hizo, pero esta sentencia en particular (\codigo{import constants, sys}) combina dos tipos diferentes de importación en una única línea: una relativa, el módulo \codigo{constants}; y una absoluta, el módulo \codigo{sys} que está preinstalado en la librería estándar de Python. En Python 2, podías combinar ambas en una única sentencia. En Python 3 no puedes. Además, el comando \codigo{2to3} no es lo suficientemente inteligente como para dividir esta sentencia en dos.

La solución consiste en dividir la sentencia manualmente:

\begin{lstlisting}[language=Python,breaklines=true]
import constants, sys
\end{lstlisting}

Se debe transformar en:

\begin{lstlisting}[language=Python,breaklines=true]
from . import constants
import sys
\end{lstlisting}

Hay diversas variaciones de este problema repartidas a lo largo de la librería \codigo{chardet}. En algunos lugares es ``\codigo{import constants, sys}''; en otros, es ``\codigo{import constants, re}''. El arreglo siempre es igual: dividir manualmente esta sentencia en dos: una para la importación relativa, la otra para la importación absoluta.

¡Sigamos!

\subsection{El nombre ``file'' no etá definido}

Y aquí vamos de nuevo, ejecutamos \codigo{test.py} para ver qué sucede con los casos de prueba\ldots

\noindent\begin{minipage}{\textwidth}
\begin{lstlisting}[breaklines=true]
C:\home\chardet> python test.py tests\*\*
tests\ascii\howto.diveintomark.org.xml
Traceback (most recent call last):
  File "test.py", line 9, in <module>
    for line in file(f, 'rb'):
NameError: name 'file' is not defined
\end{lstlisting}
\end{minipage}

\cajaTexto{\codigo{open()} es el nuevo \codigo{file()}. PapayaWhip es el nuevo negro}

Esta me sorprendió, porque he estado utilizando \codigo{file()} desde que tengo memoria. En Python 2, la función global \codigo{file()} es un alias de la función \codigo{open()}, que es el modo estándar de abrir ficheros de lectura\footnote{Ver apartado \ref{sec:leer_ficheros}, Leer contenido de ficheros de texto.}. En Python 3, no existe la función global \codigo{file()}.

La solución más simple de este problema es sustituir la llamada a la función \codigo{file} por \codigo{open}:


\begin{lstlisting}[language=Python,breaklines=true]
for line in open(f, 'rb'):
\end{lstlisting}

Y eso es todo lo que tengo que decir sobre este tema.

\subsection{No puedes usar un patrón de cadena de texto en un objeto que representa bytes}

Las cosas comienzan a ponerse interesante. Y por ``interesante'' quiero decir ``confusas como el infierno''.


\noindent\begin{minipage}{\textwidth}
\begin{lstlisting}[breaklines=true]
C:\home\chardet> python test.py tests\*\*
tests\ascii\howto.diveintomark.org.xml
Traceback (most recent call last):
  File "test.py", line 10, in <module>
    u.feed(line)
  File "C:\home\chardet\chardet\universaldetector.py", line 98, in feed
    if self._highBitDetector.search(aBuf):
TypeError: can't use a string pattern on a bytes-like object
\end{lstlisting}
\end{minipage}

Para depurar esto veamos lo que es \codigo{self.\_highBitDetector}. Está definido en el método \codigo{\_\_init\_\_} de la clase \codigo{UniversalDetector}.


\begin{lstlisting}[language=Python,breaklines=true]
class UniversalDetector:
    def __init__(self):
        self._highBitDetector = re.compile(r'[\x80-\xFF]')
\end{lstlisting}

Esto precompila una expresión regular diseñada para encontrar caracteres no ASCII que se encuentren en el rango 128-255 (0x80-0xFF). Espera, esto no es del todo cierto, necesito ser más preciso con mi terminología. Este patrón está diseñado para encontrar \emph{bytes} no ASCII en el rango 128-255.

Y ese es el problema.

En Python 2, una cadena de texto era un array de bytes cuya codificación de caracteres se mantenía separadamente. Si querías conservar la codificación de caracteres, en su lugar tenías que utilizar una cadena de caracteres Unicode (u''). Pero en Python 3 una cadena de caracteres siempre es lo que Python 2 llamaba una cadena de caracteres Unicode ---esto es, un array de caracteres Unicode (de un número de bytes posiblemente variable). Como esta expresión regular está definida por un patrón de cadena de caracteres, solo puede utilizarse para buscar en cadenas de caracteres--- es decir, un array de caracteres. Pero lo que estamos buscando no es un cadena de caracteres, es un array de bytes. Si observamos la traza del error, este sucede en \codigo{universaldetector.py}:

\begin{lstlisting}[language=Python,breaklines=true]
def feed(self, aBuf):
    .
    .
    .
    if self._mInputState == ePureAscii:
        if self._highBitDetector.search(aBuf):
\end{lstlisting}

¿Y qué es un \codigo{aBuf}? Vamos un poco más atrás a un lugar que llame a \codigo{UniversalDetector.feed()}. La prueba \codigo{test.py} lo hace:


\begin{lstlisting}[language=Python,breaklines=true]
u = UniversalDetector()
.
.
.
for line in open(f, 'rb'):
    u.feed(line)
\end{lstlisting}

Y aquí encontramos nuestra respuesta: la variable \codigo{aBuf} del método \codigo{UniversalDetector.feed()} contiene una línea leída de un fichero del disco. Observa que los parámetros utilizados para su apertura son 'rb'. 'r' es para que sea de lectura; y 'b' es para indicar 'binario'. Sin este último parámetro, este bucle \codigo{for} podría leer el fichero, línea por línea, y convertir cada una de ellas en una cadena de caracteres ---un array de caracteres Unicode--- de acuerdo a la codificación de caracteres por defecto del sistema. Pero con el parámetro 'b', este bucle \codigo{for} lee el fichero, línea a línea y almacena cada una de ellas exactamente como aparecen en el fichero, como un array de bytes. Ese array de bytes se pasa a \codigo{UniversalDetector.feed()} y, llegado el momento, se le pasa a la expresión regular precompilada, \codigo{self.\_highBitDetector}, con el fin de buscar caracteres con el bit alto activado\ldots Pero no tenemos caracteres, tenemos bytes. ¡Ups!

Necesitamos que esta expresión regular busque en un array de bytes, no de caracteres.

Una vez nos damos cuenta de ello, la solución no es difícil. Las expresiones regulares definidas con cadenas de caracteres buscan en cadenas de caracteres. Las expresiones regulares definidas con un array de bytes buscan en arrays de bytes. Para definir un patrón como un array de bytes, simplemente modificamos el tipo del argumento que usamos para definir la expresión regular:

\begin{lstlisting}[escapeinside={(*}{*)}]
  class UniversalDetector:
      def __init__(self):
(* \color{red}-         self.\_highBitDetector = re.compile(r'[\textbackslash x80-\textbackslash xFF]') *)
(* \color{red}-         self.\_escDetector = re.compile(r'(\textbackslash 033|~{)') *)
(* \color{green!50!black}+         self.\_highBitDetector = re.compile(b'[\textbackslash x80-\textbackslash xFF]') *)
(* \color{green!50!black}+         self.\_escDetector = re.compile(b'(\textbackslash 033|~{)') *)
          self._mEscCharSetProber = None
          self._mCharSetProbers = []
          self.reset()
\end{lstlisting}

La búsqueda de todo el código para localizar otros usos del módulo \codigo{re} encuentra dos sitios más, ambos en \codigo{charsetprober.py}. De nuevo, el código define las expresiones regulares como cadenas de caracteres pero las está ejecutando sobre \codigo{aBuf}, que es un array de bytes. La solución es la misma: definir los patrones de la expresión regular como un array de bytes.


\begin{lstlisting}[escapeinside={(*}{*)}]
  class CharSetProber:
      .
      .
      .
      def filter_high_bit_only(self, aBuf):
(* \color{red}-         aBuf = re.sub(r'([\textbackslash x00-\textbackslash x7F])+', ' ', aBuf) *)
(* \color{green!50!black}+         aBuf = re.sub(b'([\textbackslash x00-\textbackslash x7F])+', b' ', aBuf) *)
          return aBuf
    
      def filter_without_english_letters(self, aBuf):
(* \color{red}-         aBuf = re.sub(r'([A-Za-z])+', ' ', aBuf) *)
(* \color{green!50!black}+         aBuf = re.sub(b'([A-Za-z])+', b' ', aBuf) *)
          return aBuf
\end{lstlisting}
