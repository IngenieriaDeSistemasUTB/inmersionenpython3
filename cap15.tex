% ch15.tex
% This work is licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 License.
% To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-sa/3.0/nz
% or send a letter to Creative Commons, 171 Second Street, Suite 300, San Francisco, California, 94105, USA.

\chapter{Caso de estudio: migrar chardet a Python 3}\label{ch:chardet}

\noindent Nivel de dificultad:\diflllll

\begin{citaCap}
    ``Palabras, palabras. Son todo lo que tenemos para continuar.'' \\
        ---\emph{Rosencrantz y Guildenstern han muerto\footnote{\href{http://www.imdb.com/title/tt0100519/quotes}{http://www.imdb.com/title/tt0100519/quotes}}}
\end{citaCap}

\section{Inmersión}

Pregunta: ¿Cuál es la primera causa de texto ininteligible en la web, en tu buzón de entrada, y en cualquier ordenador? Es la codificación de caracteres. En el capítulo \ref{ch:cadenas}, sobre las cadenas de caracteres, hablé sobre la historia de la codificación de caracteres y la creación de Unicode, la “codificación de caracteres para gobernarlas a todas”. Me gustaría no volver a ver ningún carácter ininteligible en una página web nunca más, porque todos los sistemas de creación de contenidos almacenaran información precisa sobre la codificación de caracteres; todos los protocolos de transferencia de información fueran conscientes de la existencia de Unicode, y todos los sistemas manejaran el texto con fidelidad al convertirlo entre diferentes codificaciones.

También me gustaría un pony.

Un pony de Unicode.

Un Unipony, podría decir.

Creo que me decantaré por la detección automática de la codificación de caracteres.

\section{¿En qué consiste la detección automática?}

Consiste en tomar una secuencia de bytes con una codificación de caracteres desconocida, e intentar determinar cuál es con el fin de poder leer el texto representado. Es como craquear un código cuando no dispones de la clave de decodificación.

\subsection{¿Eso no es imposible?}

En general: sí. Sin embargo, algunas codificaciones están optimizadas para idiomas específicos, y los idiomas no son aleatorios. Algunas secuencias de caracteres se repiten constantemente, mientras que otras no tienen sentido. Una persona con un inglés fluido que abre un períodico y lee “txzqJv 2!dasd0a QqdKjvz” reconoce instantáneamente que eso no es inglés (incluso aunque esté compuesto de letras ingles). Mediante el estudio de grandes cantidades de texto “típico”, un algoritmo puede simular esta clase de “lectura” fluida y proponer la codificación de caracteres en la que puede encontrar un texto.

En otras palabras, la detección de la codificación de caracteres es en realidad la detección del idioma, combinada con el conocimiento de las codificaciones de caracteres que tienden a utilizar.

\subsection{¿Existe tal algoritmo?}

Resulta que sí. Todos los navegadores tienen autodetección de la codificación de caracteres ya que la web está llena de páginas que no tienen ninguna información al respecto. Mozilla Firefox contiene una librería de autodetección de la codificación de caracteres\footnote{\href{http://lxr.mozilla.org/seamonkey/source/extensions/universalchardet/src/base/}{http://lxr.mozilla.org/seamonkey/source/extensions/universalchardet/src/base/}} que es de código abierto. Yo porté dicha librería a Python 2 bajo el módulo denominado \codigo{chardet}. Este capítulo describe los pasos a través del proceso de conversión del módulo \codigo{charffdet} de Python 2 a Python 3.

\section{Introducción al módulo \codigo{chardet}}

\cajaTexto{La detección de la codificación de caracteres es en realidad la detección del lenguaje con dificultades.}

Antes de convertir el código, ayudaría que entendieras cómo funciona. Esta es una guía breve a través del propio código. La librería \codigo{chardet} es demasiado larga para incluirla completa aquí, pero puedes descargarla de \codigo{chardet.feedparser.org}\footnote{\href{http://chardet.feedparser.org/download/}{http://chardet.feedparser.org/download/}}.


El punto de entrada principal para el algoritmo de detección es \codigo{universaldetector.py}, que contiene una clase, \codigo{UniversalDetector} \footnote{Podrías pensar que el punto de entrada principal es la función \codigo{detect} en \codigo{chardet/\_\_init\_\_.py}, pero en realidad esto es una función de conveniencia para crear un objeto \codigo{UniversalDetector} , llamarlo, y devolver su resultado}.

\codigo{UniversalDetector} puede manejar cinco categorías de codificaciones de caracteres:

\begin{enumerate}
  \item \codigo{UTF-N} con \codigo{BOM}\footnote{Ver capítulo \ref{ch:cadenas} sobre Cadenas de Caracteres}, con las variantes ``Big-Endian'' y ``Little-Endian'' de \codigo{UTF-16}, y todas las variantes de 4-bytes de \codigo{UTF-16}.

  \item Codificaciones de escape, compatibles con \codigo{ASCII} de 7-bits, 
    en las que los caracteres \codigo{no-ASCII} comienzan con una secuencia de escape. Por ejemplo: \codigo{ISO-2022-JP} (japonés) y \codigo{HZ-GB-2312}(chino).

  \item Codificaciones multibyte, en las que cada carácter se representa por un número variable de bytes. Por ejemplo: \codigo{BIG5} (chino), \codigo{SHIFT\_JIS} (japonés), \codigo{EUC-KR} (coreano), y \codigo{UTF-8} sin \codigo{BOM}.

  \item Codificaciones de un solo byte, en las que cada carácter se representa por un único byte. Por ejemplo: \codigo{KOI8-R} (ruso), \codigo{WINDOWS-1255} (hebreo), y \codigo{TIS-620} (thai).

  \item \codigo{WINDOWS-1252}, que se utiliza fundamentalmente en Microsoft Windows por los mandos intermedios que no distinguen una codificación de caracteres de un agujero en el suelo.

\end{enumerate}

\subsection{\codigo{UTF-N} con \codigo{BOM}}

Si el texto comienza con una marca \codigo{BOM}, se puede asumir de forma razonable que está codificado en \codigo{UTF-8}, \codigo{UTF-16} o \codigo{UTF-32} (Precisamente el \codigo{BOM} sirve para indicar cuál de ellos es). Esto es manejado por la propia clase \codigo{UniversalDetector}, que retorna el resultado inmediatamente sin proceso adicional.

\subsection{Codificaciones con código de escape}

Si el texto contiene una cadena de caracteres de escape reconocible podría indicar que se encuentra en una de las codificaciones que se basan en ello. \codigo{UniversalDetector} crea un objeto \codigo{EscCharSetProber} (definido en el \codigo{escprober.py}) y le pasa el texto.

\codigo{ExcCharSetProber} crea una serie de máquinas de estado, basadas en los modelos definidos en \codigo{escsm.py}): \codigo{HZ-GB-2312}, \codigo{ISO-2022-CN}, \codigo{ISO-2022-JP}, y \codigo{ISO-2022-KR}. \codigo{EscCharSetProber} alimenta el texto a cada una de las máquinas de estado, byte a byte. Si alguna de ellas finaliza identificando la codificación, \codigo{EscCharSetProber} finaliza devolviendo el resultado a \codigo{UniversalDetector}, que lo retorna al llamante. Si cualquiera de las máquinas de estado alcanza una secuencia ilegal, finaliza su ejecución y se sigue la ejecución con la siguiente máquina de estados.

\subsection{Codificaciones multibyte}

Asumiendo que no existe \codigo{BOM}, \codigo{UniversalDetector} chequea si el texto contiene algún carácter con bits altos activados. Si es así, crea una serie de ``sondas'' para detectar codificaciones multibyte, de un byte y, como último recurso, \codigo{windos-1252}.

La sonda de codificaciones multibyte, \codigo{MBCSGroupProber} (definida en \codigo{mbcsgroupprober.py}), es en realidad un envoltorio que gestiona un grupo de sondas, una para cada tipo de codificación multibyte: \codigo{BIG5}, \codigo{GB2313}, \codigo{EUC-TW}, \codigo{EUC-KR}, \codigo{EUC-JP}, \codigo{SHIFT\_JIS}, y \codigo{UTF-8}. \codigo{MBCSGroupProber} alimenta el texto a cada una de las sondas específicas y chequea el resultado. Si una sonda reporta una secuencia de bytes ilegal, se elimina de la búsqueda (cualquier llamada posterior a \codigo{UniversalDetector.feed()} para este texto se saltará a esta sonda). Si una sonda informa que está segura razonablemente de que ha detectado la codificación, \codigo{MBCSGroupProber} informa del resultado positivo a \codigo{UniversalDetector}, que devuelve el resultado al llamante.

La mayoría de las sondas de la codificación multibyte heredan de \codigo{MultiByteCharSetProber} (definida en \codigo{mbcharsetprober.py}, y simplemente activan la máquina de estados y analizador de distribución apropiado y dejan a la clase \codigo{MultiByteCharSetProber} hacer el resto del trabajo. \codigo{MultiByteCharSetProber} recorre el texto a traves de la máquina de estados específica byte a byte, para buscar secuencias de caracteres que pudieran indicar de forma concluyente un resultado positivo o negativo. Al mismo tiempo, \codigo{MultiByteCharSetProber} alimenta el texto a un analizador de distribución específico de cada codificación de caracteres.

Los analizadores de distribución (definidos en \codigo{chardistribution.py}) utilizan modelos específicos para cada idioma que tienen en cuenta los caracteres más frecuentes en cada uno de ellos. Cuando \codigo{MultiByteCharSetProber} ha alimentado suficiente texto a los analizadores, calcula el grado de confianza basándose en el número de caracteres más frecuentes, el número total de caracteres, y el ratio de distribución específico de cada lenguaje. Si el grado de confianza es suficientemente algo, \codigo{MultiByteCharSetProber} devuelve el resultado a \codigo{MBCSGroupProber}, que lo devuelve a \codigo{UniversalDetector}, quien, a su vez, lo devuelve al llamante.

El caso del idioma Japonés es más difícil. Un análisis de distribución monocarácter no es siempre suficiente para distinguir entre \codigo{EUC-JP} y \codigo{SHIFT\_JIS}, por ello la sonda \codigo{SJISProber} (definida en \codigo{sjisprober.py} también utiliza un análisis de distribución de dos caracteres. \codigo{SJISContextAnalysis} y \codigo{EUCJPContextAnalysis} (ambos definidos en \codigo{jpcntx.py}) comprueban la frecuencia en el texto de los caracteres del silabario Hiragana. Cuando se ha procesado texto suficiente, devuelven el grado de confianza a \codigo{SJISProber}, que chequea ambos analizadores y devuelve aquél de mayor grado de confianza a \codigo{MBCSGroupProber}.

\subsection{Codificaciones de un solo byte}

\cajaTexto{En serio, dónde está mi pony unicode}

La sonda de codificaciones de un solo byte, \codigo{SBCSGroupProber} (definida en \codigo{sbcsgroupprober.py}), también es un envoltorio que gestiona el grupo de sondas que existen para cada combinación de idioma y codificación de un solo byte: \codigo{windows-1251}, \codigo{KOI8-R}, \codigo{ISO-8859-5}, \codigo{MacCyrillic}, \codigo{IBM855}, y \codigo{IBM866} (ruso); \codigo{ISO-8859-7} y \codigo{windows-1253} (griego); \codigo{ISO-8859-5} y \codigo{windows-1251} (búlgaro); \codigo{ISO-8859-2} y \codigo{windows-1250} (húngaro); \codigo{TIS-620} (Thai); \codigo{windows-1255} e \codigo{ISO-8859-8} (Hebreo).

\codigo{SBCSGroupProber} alimenta el texto a cada uno de estas sondas específicas y comprueba sus resultados. Las sondas están implementadas con un única clase \codigo{SingleByteCharSetProber} (definida en \codigo{sbcharsetprober.py}, que utiliza como parámetro del constructor el modelo del idioma. Este define la frecuencia de aparición de las diferentes secuencias de dos caracteres en un texto típico. \codigo{SingleByteByteCharSetProber} procesa el texto contando las secuencias de dos caracteres más frecuentes. Cuando se ha procesado suficiente texto, calcula el nivel de confianza basado en el número de dichas secuencias, el número total de caracteres, y la distribución específica del idioma.

El hebreo se maneja como un caso especial. Si el texto parece hebreo, basado en el análisis de distribución de dos caracteres, \codigo{HebrewProber} (definido en \codigo{hebrewprober.py} intenta distinguir entre hebreo visual (en el que el texto está realmente almacenado ``hacia atrás'' línea a línea, y luego se muestra directamente para que pueda leerse de derecha a izquierda) y hebreo lógico (en el que el texto se almacena en el orden de lectura y luego se visualiza de derecha a izquierda por el sistema). Debido a que ciertos caracteres se codifican de forma diferente en función de que aparezcan en medio de una palabra o al final, podemos intentar adivinar de forma razonable la dirección del texto fuente, y devolver la codificación correcta (\codigo{windows-1255} en el caso de hebreo lógico, o \codigo{ISO-8859-8} para el hebreo visual).

\subsection{windows-1252}

Si \codigo{UniversalDetector} encuentra en el texto un carácter con el bit alto activado, pero ninguno de las sondas anteriores devuelve un resultado fiable, crea un objeto \codigo{Latin1Prober} (definido en \codigo{latin1prober.py}) para intentar detectar texto en inglés en una codificación \codigo{windows-1252}. Esta no es fiable (inherentemente), porque las letras en inglés se codifican de la misma forma en diferentes codificaciones. La única forma de distinguir \codigo{windows-1252} es a través de algunos símbolos comúnmente utilizados como las comillas inteligentes, los apostrofos, símbolos de copyright, y otros similares. \codigo{Latin1Prober} reduce su estimación del nivel de confianza para permitir que le ``ganen'' otras sondas más precisas, si es posible.

\section{Ejecutando \codigo{2to3}}

Vamos a migrar el módulo \codigo{chardet} de Python 2 a Python 3. Este último trae una utilidad denominada \codigo{2to3}, que toma el código fuente de Python 2 como entrada y lo convierte de forma automática a Python 3. En algunos casos es fácil ---Cambiar una función de librería que fue renombrada o movida a otro módulo---, pero en otros casos puede ser muy complejo. Para entender lo que puede llegar a hacer, lee el apéndice \ref{ap:porting}, Migrando código a Python 3 con \codigo{2to3}. En este capítulo comenzaremos ejecutando \codigo{2to3} en el paquete \codigo{chardet}, verás que aún quedará bastante trabajo que hacer después de que las herramientas automatizadas hayan aplicado su magia.

El paquete \codigo{chardet} está dividido en varios ficheros, todos en el mismo directorio. El script \codigo{2to3} facilita la conversión de varios ficheros a la vez: basta con pasarle el nombre del directorio como parámetro y \codigo{2to3} convertirá cada uno de los ficheros que contenga.


\begin{lstlisting}[escapeinside={(*}{*)}]
C:\home\chardet> python c:\Python30\Tools\Scripts\2to3.py -w chardet\
RefactoringTool: Skipping implicit fixer: buffer
RefactoringTool: Skipping implicit fixer: idioms
RefactoringTool: Skipping implicit fixer: set_literal
RefactoringTool: Skipping implicit fixer: ws_comma
--- chardet\__init__.py (original)
+++ chardet\__init__.py (refactored)
@@ -18,7 +18,7 @@
 __version__ = "1.0.1"

 def detect(aBuf):
(* \color{red}-    import universaldetector *)
(* \color{green!50!black}+    from . import universaldetector *)
     u = universaldetector.UniversalDetector()
     u.reset()
     u.feed(aBuf)
--- chardet\big5prober.py (original)
+++ chardet\textbackslash big5prober.py (refactored)
@@ -25,10 +25,10 @@
 # 02110-1301  USA
 ######################### END LICENSE BLOCK #########################

(* \color{red}-from mbcharsetprober import MultiByteCharSetProber *)
(* \color{red}-from codingstatemachine import CodingStateMachine *)
(* \color{red}-from chardistribution import Big5DistributionAnalysis *)
(* \color{red}-from mbcssm import Big5SMModel *)
(* \color{green!50!black}+from .mbcharsetprober import MultiByteCharSetProber *)
(* \color{green!50!black}+from .codingstatemachine import CodingStateMachine *)
(* \color{green!50!black}+from .chardistribution import Big5DistributionAnalysis *)
(* \color{green!50!black}+from .mbcssm import Big5SMModel *)

 class Big5Prober(MultiByteCharSetProber):
     def __init__(self):
--- chardet\chardistribution.py (original)
+++ chardet\chardistribution.py (refactored)
@@ -25,12 +25,12 @@
 # 02110-1301  USA
 ######################### END LICENSE BLOCK #########################

(* \color{red}-import constants *)
(* \color{red}-from euctwfreq import EUCTWCharToFreqOrder,  *)
(* \color{red}      EUCTW\_TABLE\_SIZE, EUCTW\_TYPICAL\_DISTRIBUTION\_RATIO *)
(* \color{red}-from euckrfreq import EUCKRCharToFreqOrder, *)
(* \color{red}      EUCKR\_TABLE\_SIZE, EUCKR\_TYPICAL\_DISTRIBUTION\_RATIO *)
(* \color{red}-from gb2312freq import GB2312CharToFreqOrder, *)
(* \color{red}      GB2312\_TABLE\_SIZE, GB2312\_TYPICAL\_DISTRIBUTION\_RATIO *)
(* \color{red}-from big5freq import Big5CharToFreqOrder, *)
(* \color{red}      BIG5\_TABLE\_SIZE, BIG5\_TYPICAL\_DISTRIBUTION\_RATIO *)
(* \color{red}-from jisfreq import JISCharToFreqOrder, *)
(* \color{red}      JIS\_TABLE\_SIZE, JIS\_TYPICAL\_DISTRIBUTION\_RATIO *)
(* \color{green!50!black}+from . import constants *)
(* \color{green!50!black}+from .euctwfreq import EUCTWCharToFreqOrder, *)
(* \color{green!50!black}      EUCTW\_TABLE\_SIZE, EUCTW\_TYPICAL\_DISTRIBUTION\_RATIO *)
(* \color{green!50!black}+from .euckrfreq import EUCKRCharToFreqOrder, *)
(* \color{green!50!black}      EUCKR\_TABLE\_SIZE, EUCKR\_TYPICAL\_DISTRIBUTION\_RATIO *)
(* \color{green!50!black}+from .gb2312freq import GB2312CharToFreqOrder, *)
(* \color{green!50!black}      GB2312\_TABLE\_SIZE, GB2312\_TYPICAL\_DISTRIBUTION\_RATIO *)
(* \color{green!50!black}+from .big5freq import Big5CharToFreqOrder,  *)
(* \color{green!50!black}      BIG5\_TABLE\_SIZE, BIG5\_TYPICAL\_DISTRIBUTION\_RATIO *)
(* \color{green!50!black}+from .jisfreq import JISCharToFreqOrder,  *)
(* \color{green!50!black}      JIS\_TABLE\_SIZE, JIS\_TYPICAL\_DISTRIBUTION\_RATIO *)

 ENOUGH_DATA_THRESHOLD = 1024
 SURE_YES = 0.99
.
.
(* \color{yellow!75!black}. (Durante un rato va sacando mensajes de este tipo) *)
.
.
RefactoringTool: Files that were modified:
RefactoringTool: chardet\__init__.py
RefactoringTool: chardet\big5prober.py
RefactoringTool: chardet\chardistribution.py
RefactoringTool: chardet\charsetgroupprober.py
RefactoringTool: chardet\codingstatemachine.py
RefactoringTool: chardet\constants.py
RefactoringTool: chardet\escprober.py
RefactoringTool: chardet\escsm.py
RefactoringTool: chardet\eucjpprober.py
RefactoringTool: chardet\euckrprober.py
RefactoringTool: chardet\euctwprober.py
RefactoringTool: chardet\gb2312prober.py
RefactoringTool: chardet\hebrewprober.py
RefactoringTool: chardet\jpcntx.py
RefactoringTool: chardet\langbulgarianmodel.py
RefactoringTool: chardet\langcyrillicmodel.py
RefactoringTool: chardet\langgreekmodel.py
RefactoringTool: chardet\langhebrewmodel.py
RefactoringTool: chardet\langhungarianmodel.py
RefactoringTool: chardet\langthaimodel.py
RefactoringTool: chardet\latin1prober.py
RefactoringTool: chardet\mbcharsetprober.py
RefactoringTool: chardet\mbcsgroupprober.py
RefactoringTool: chardet\mbcssm.py
RefactoringTool: chardet\sbcharsetprober.py
RefactoringTool: chardet\sbcsgroupprober.py
RefactoringTool: chardet\sjisprober.py
RefactoringTool: chardet\universaldetector.py
RefactoringTool: chardet\utf8prober.py

Now run the 2to3 script on the testing harness, test.py.

C:\home\chardet> python c:\Python30\Tools\Scripts\2to3.py -w test.py
RefactoringTool: Skipping implicit fixer: buffer
RefactoringTool: Skipping implicit fixer: idioms
RefactoringTool: Skipping implicit fixer: set_literal
RefactoringTool: Skipping implicit fixer: ws_comma
--- test.py (original)
+++ test.py (refactored)
@@ -4,7 +4,7 @@
 count = 0
 u = UniversalDetector()
 for f in glob.glob(sys.argv[1]):
(* \color{red}-    print f.ljust(60), *)
(* \color{green!50!black}+    print(f.ljust(60), end=' ') *)
     u.reset()
     for line in file(f, 'rb'):
         u.feed(line)
@@ -12,8 +12,8 @@
     u.close()
     result = u.result
     if result['encoding']:
(* \color{red}-        print result['encoding'], 'with confidence', result['confidence'] *)
(* \color{green!50!black}+        print(result['encoding'], 'with confidence', result['confidence']) *)
     else:
(* \color{red}-        print '******** no result' *)
(* \color{green!50!black}+        print('******** no result') *)
     count += 1
(* \color{red}-print count, 'tests' *)
(* \color{green!50!black}+print(count, 'tests') *)
RefactoringTool: Files that were modified:
RefactoringTool: test.py
\end{lstlisting}

Bueno, no fue para tanto. Solo se han convertido unos pocos imports y sentencias print. Por cierto, cuál era el problema con todas las sentencias import. Para contestar a esto, tienes que entender que el módulo \codigo{chardet} estaba dividido en múltiples ficheros.

\section{Una breve disgresión sobre los módulos \emph{multifichero}}

\codigo{chardet} es un módulo multifichero. Podría haber elegido poner todo el código en uno solo (denominado \codigo{chardet.py}, pero no lo hice. En vez de eso, creé un directorio (denominado \codigo{chardet}), luego creé un fichero \codigo{\_\_init\_\_.py} en él, \emph{con ello se asume que todos los ficheros de este directorio son parte del mismo módulo}. El nombre del módulo es el nombre del directorio. Los ficheros que están dentro del directorio pueden referenciar a otros ficheros dentro del mismo, o incluso en subdirectorios (más sobre esto en un minuto). Pero la colección completa de ficheros se presenta para otro código de Python como un único módulo ---como si las funciones y las clases se encontrasen un único fichero de extensión \codigo{.py}.

¿Qué contiene el fichero \codigo{\_\_init\_\_.py}? Nada. Todo. Algo intermedio. El fichero \codigo{\_\_init\_\_.py} no necesita definir nada. Puede ser un fichero vacío. Pero también se puede utilizar para definir en él las funciones que sean punto de entrada a tu módulo. O puedes poner todas las funciones en él. O todas, salvo una\ldots

\cajaTextoAncho{Un directorio con un fichero \codigo{\_\_init\_\_.py} siempre se trata como un módulo multifichero. Sin un fichero \codigo{\_\_init\_\_.py}, un directorio no contiene más que un conjunto de ficheros \codigo{.py} sin relación alguna}

Veamos como funciona esto en la práctica.

\begin{lstlisting}[breaklines=true]
>>> import chardet
>>> dir(chardet)
['__builtins__', '__doc__', '__file__', '__name__',
 '__package__', '__path__', '__version__', 'detect']
>>> chardet
<module 'chardet' from 'C:\Python31\lib\site-packages\chardet\__init__.py'>
\end{lstlisting}

\begin{enumerate}
  \item \emph{Línea 2:} aparte de los atributos de clase habituales, lo único que hay en el módulo \codigo{chardet} es la función \codigo{detect()}.
  \item \emph{Línea 5:} aquí aparece la primera pista de que el módulo \codigo{chardet} está formado por más de un fichero; el ``módulo'' se muestra como procedente del fichero \codigo{\_\_init\_\_.py} del directorio \codigo{chardet/}.
\end{enumerate}

Veamos el contenido del fichero \codigo{\_\_init\_\_.py}.


\begin{lstlisting}[language=Python,breaklines=true]
def detect(aBuf):                              
    from . import universaldetector            
    u = universaldetector.UniversalDetector()
    u.reset()
    u.feed(aBuf)
    u.close()
    return u.result
\end{lstlisting}

\begin{enumerate}
  \item \emph{Línea 1:} El fichero define la función \codigo{detect()}, que es el punto de entrada principal del módulo \codigo{chardet}.
  \item \emph{Línea 2:} Esta función tiene poquísimo código. En realidad, todo lo que hace es importar el módulo \codigo{universaldetector} y comenzar a usarlo. ¿Pero dónde está definido \codigo{universaldetector}?
\end{enumerate}

La respuesta se encuentra en esa extraña sentencia \codigo{import}.

\begin{lstlisting}[language=Python,breaklines=true]
from . import universaldetector
\end{lstlisting}

Traducido, significa que ``se importe el módulo \codigo{universaldetector}; que está en el mismo directorio en el que estoy yo (el fichero \codigo{chardet/\_\_init\_\_.py})''. A esto se le denomina \emph{importación relativa}. Es la forma en que se localizan entre sí los ficheros que se encuentran en un módulo multifichero, sin preocuparse de conflictos de denominación con otros módulos que puedas haber instalado en tu \emph{camino de búsqueda de módulos}\footnote{Ver la sección \ref{sc:search_path}, sobre este tema.}. Esta sentencia \codigo{import} solamente buscará a \codigo{universaldetector} dentro del propio directorio \codigo{chardet}.

Estos dos conceptos ---\codigo{\_\_init\_\_.py} e importación relativa--- permiten que puedas dividir un módulo en las piezas que quieras. El módulo \codigo{chardet} consta de 36 ficheros \codigo{.py}, ¡36!. Aún así, para utilizarlo únicamente necesitas usar \codigo{import chardet}, y luego llamar a la función \codigo{chardet.detect()}. Para tu código es transparente dónde está definida la función, si utiliza una importación relativa de \codigo{universaldetector} y que este, a su vez, utiliza cinco importaciones relativas de otros tantos ficheros, todos contenidos en el directorio \codigo{chardet/}.

\cajaTextoAncho{Si alguna vez te encuentras en la necesidad de escribir una librería grande en Python (o más problemente, cuando te des cuenta de que tu pequeña librería ha crecido hasta convertirse en grande), tómate tu tiempo en refactorizarla en un módulo multifichero. Es una de las muchas cosas en las que Python es muy bueno, así que aprovéchate de ello.}

\section{Arreglando lo que \codigo{2to3} no puede}

\subsection{\codigo{False} es sintaxis inválida}

\cajaTexto{¿Tienes pruebas? ¿No?}

Ahora vamos a las pruebas reales: ejecutar la suite de pruebas. Puesto que la suite de pruebas esta diseñada para cubrir todos los caminos posibles del código, es una buena manera de probar el código migrado para estar seguro de que no hay errores acechando en algún rincón.

\noindent\begin{minipage}{\textwidth}
\begin{lstlisting}[breaklines=true]
C:\home\chardet> python test.py tests\*\*
Traceback (most recent call last):
  File "test.py", line 1, in <module>
    from chardet.universaldetector import UniversalDetector
  File "C:\home\chardet\chardet\universaldetector.py", line 51
    self.done = constants.False
                              ^
SyntaxError: invalid syntax
\end{lstlisting}
\end{minipage}

Vaya, un pequeño fallo. En Python 3, \codigo{False} es una palabra reservada, así que no la puedes utilizar como nombre de una variable. Vamos a mirar \codigo{constants.py} para ver dónde está definida. Aquí está la versión original del fichero antes de que \codigo{2to3} lo cambiara:


\begin{lstlisting}[language=Python,breaklines=true]
import __builtin__
if not hasattr(__builtin__, 'False'):
    False = 0
    True = 1
else:
    False = __builtin__.False
    True = __builtin__.True
\end{lstlisting}

Este código está diseñado para que funcione con versiones antiguas de \mbox{Python 2}. Antes de Python 2.3 no existía el tipo \codigo{bool}. El código detecta la ausencia de las constantes \codigo{True} y \codigo{False} y las define si es necesario.

Sin embargo, en Python 3 siempre existe el tipo \codigo{bool}, por lo que este código es innecesario. La solución más simple pasa por sustituir todas las instancias de \codigo{constants.True} y \codigo{constants.False} por \codigo{True} y \codigo{False}, respectivamente. Y borrar este fichero.

Así, esta línea del fichero \codigo{universaldetector.py}:

\begin{lstlisting}[language=Python,breaklines=true]
self.done = constants.False
\end{lstlisting}

Se convierte en:

\begin{lstlisting}[language=Python,breaklines=true]
self.done = False
\end{lstlisting}

¡Ah! ¿No es satisfactorio? El código queda más corto y más legible así.

\subsection{No hay ningún módulo denominado \codigo{constants}}

Hora de volver a ejecutar \codigo{test.py} y ver hasta donde llega.

\noindent\begin{minipage}{\textwidth}
\begin{lstlisting}[breaklines=true]
C:\home\chardet> python test.py tests\*\*
Traceback (most recent call last):
  File "test.py", line 1, in <module>
    from chardet.universaldetector import UniversalDetector
  File "C:\home\chardet\chardet\universaldetector.py", line 29, in <module>
    import constants, sys
ImportError: No module named constants
\end{lstlisting}
\end{minipage}

¿Qué dices? ¿Que no hay un módulo denominado \codigo{constants}? Desde luego que sí. Está ahí mismo, en \codigo{chardet/constants.py}.

¿Recuerdas que el comando \codigo{2to3} arregló muchas sentencias \codigo{import}? Esta librería tiene muchas importaciones relativas ---esto es, módulos que importan a otros módulos dentro de la misma librería--- pero la lógica que gobierna las mismas ha cambiado en Python 3. En Python 2, podías escribir \codigo{import constants} y el primer lugar en el que se busca es en el directorio \codigo{chardet/}. En Python 3, todas las sentencias \codigo{import} son absolutas por defecto. Si quieres una relativa hay que ser explícito:

\begin{lstlisting}[language=Python,breaklines=true]
from . import constants
\end{lstlisting}

Pero espera, ¿No se supone que \codigo{2to3} tenía que haber resuelto esto por ti? Bueno, lo hizo, pero esta sentencia en particular (\codigo{import constants, sys}) combina dos tipos diferentes de importación en una única línea: una relativa, el módulo \codigo{constants}; y una absoluta, el módulo \codigo{sys} que está preinstalado en la librería estándar de Python. En Python 2, podías combinar ambas en una única sentencia. En Python 3 no puedes. Además, el comando \codigo{2to3} no es lo suficientemente inteligente como para dividir esta sentencia en dos.

La solución consiste en dividir la sentencia manualmente:

\begin{lstlisting}[language=Python,breaklines=true]
import constants, sys
\end{lstlisting}

Se debe transformar en:

\begin{lstlisting}[language=Python,breaklines=true]
from . import constants
import sys
\end{lstlisting}

Hay diversas variaciones de este problema repartidas a lo largo de la librería \codigo{chardet}. En algunos lugares es ``\codigo{import constants, sys}''; en otros, es ``\codigo{import constants, re}''. El arreglo siempre es igual: dividir manualmente esta sentencia en dos: una para la importación relativa, la otra para la importación absoluta.

¡Sigamos!

\subsection{El nombre ``file'' no etá definido}

Y aquí vamos de nuevo, ejecutamos \codigo{test.py} para ver qué sucede con los casos de prueba\ldots

\noindent\begin{minipage}{\textwidth}
\begin{lstlisting}[breaklines=true]
C:\home\chardet> python test.py tests\*\*
tests\ascii\howto.diveintomark.org.xml
Traceback (most recent call last):
  File "test.py", line 9, in <module>
    for line in file(f, 'rb'):
NameError: name 'file' is not defined
\end{lstlisting}
\end{minipage}

\cajaTexto{\codigo{open()} es el nuevo \codigo{file()}. PapayaWhip es el nuevo negro}

Esta me sorprendió, porque he estado utilizando \codigo{file()} desde que tengo memoria. En Python 2, la función global \codigo{file()} es un alias de la función \codigo{open()}, que es el modo estándar de abrir ficheros de lectura\footnote{Ver apartado \ref{sec:leer_ficheros}, Leer contenido de ficheros de texto.}. En Python 3, no existe la función global \codigo{file()}.

La solución más simple de este problema es sustituir la llamada a la función \codigo{file} por \codigo{open}:


\begin{lstlisting}[language=Python,breaklines=true]
for line in open(f, 'rb'):
\end{lstlisting}

Y eso es todo lo que tengo que decir sobre este tema.

\subsection{No puedes usar un patrón de cadena de texto en un objeto que representa bytes}

Las cosas comienzan a ponerse interesante. Y por ``interesante'' quiero decir ``confusas como el infierno''.


\noindent\begin{minipage}{\textwidth}
\begin{lstlisting}[breaklines=true]
C:\home\chardet> python test.py tests\*\*
tests\ascii\howto.diveintomark.org.xml
Traceback (most recent call last):
  File "test.py", line 10, in <module>
    u.feed(line)
  File "C:\home\chardet\chardet\universaldetector.py", line 98, in feed
    if self._highBitDetector.search(aBuf):
TypeError: can't use a string pattern on a bytes-like object
\end{lstlisting}
\end{minipage}

Para depurar esto veamos lo que es \codigo{self.\_highBitDetector}. Está definido en el método \codigo{\_\_init\_\_} de la clase \codigo{UniversalDetector}.


\begin{lstlisting}[language=Python,breaklines=true]
class UniversalDetector:
    def __init__(self):
        self._highBitDetector = re.compile(r'[\x80-\xFF]')
\end{lstlisting}

Esto precompila una expresión regular diseñada para encontrar caracteres no ASCII que se encuentren en el rango 128-255 (0x80-0xFF). Espera, esto no es del todo cierto, necesito ser más preciso con mi terminología. Este patrón está diseñado para encontrar \emph{bytes} no ASCII en el rango 128-255.

Y ese es el problema.

En Python 2, una cadena de texto era un array de bytes cuya codificación de caracteres se mantenía separadamente. Si querías conservar la codificación de caracteres, en su lugar tenías que utilizar una cadena de caracteres Unicode (u''). Pero en Python 3 una cadena de caracteres siempre es lo que Python 2 llamaba una cadena de caracteres Unicode ---esto es, un array de caracteres Unicode (de un número de bytes posiblemente variable). Como esta expresión regular está definida por un patrón de cadena de caracteres, solo puede utilizarse para buscar en cadenas de caracteres--- es decir, un array de caracteres. Pero lo que estamos buscando no es un cadena de caracteres, es un array de bytes. Si observamos la traza del error, este sucede en \codigo{universaldetector.py}:

\begin{lstlisting}[language=Python,breaklines=true]
def feed(self, aBuf):
    .
    .
    .
    if self._mInputState == ePureAscii:
        if self._highBitDetector.search(aBuf):
\end{lstlisting}

¿Y qué es un \codigo{aBuf}? Vamos un poco más atrás a un lugar que llame a \codigo{UniversalDetector.feed()}. La prueba \codigo{test.py} lo hace:


\begin{lstlisting}[language=Python,breaklines=true]
u = UniversalDetector()
.
.
.
for line in open(f, 'rb'):
    u.feed(line)
\end{lstlisting}

Y aquí encontramos nuestra respuesta: la variable \codigo{aBuf} del método \codigo{UniversalDetector.feed()} contiene una línea leída de un fichero del disco. Observa que los parámetros utilizados para su apertura son 'rb'. 'r' es para que sea de lectura; y 'b' es para indicar 'binario'. Sin este último parámetro, este bucle \codigo{for} podría leer el fichero, línea por línea, y convertir cada una de ellas en una cadena de caracteres ---un array de caracteres Unicode--- de acuerdo a la codificación de caracteres por defecto del sistema. Pero con el parámetro 'b', este bucle \codigo{for} lee el fichero, línea a línea y almacena cada una de ellas exactamente como aparecen en el fichero, como un array de bytes. Ese array de bytes se pasa a \codigo{UniversalDetector.feed()} y, llegado el momento, se le pasa a la expresión regular precompilada, \codigo{self.\_highBitDetector}, con el fin de buscar caracteres con el bit alto activado\ldots Pero no tenemos caracteres, tenemos bytes. ¡Ups!

Necesitamos que esta expresión regular busque en un array de bytes, no de caracteres.

Una vez nos damos cuenta de ello, la solución no es difícil. Las expresiones regulares definidas con cadenas de caracteres buscan en cadenas de caracteres. Las expresiones regulares definidas con un array de bytes buscan en arrays de bytes. Para definir un patrón como un array de bytes, simplemente modificamos el tipo del argumento que usamos para definir la expresión regular:

\begin{lstlisting}[escapeinside={(*}{*)}]
  class UniversalDetector:
      def __init__(self):
(* \color{red}-         self.\_highBitDetector = re.compile(r'[\textbackslash x80-\textbackslash xFF]') *)
(* \color{red}-         self.\_escDetector = re.compile(r'(\textbackslash 033|~{)') *)
(* \color{green!50!black}+         self.\_highBitDetector = re.compile(b'[\textbackslash x80-\textbackslash xFF]') *)
(* \color{green!50!black}+         self.\_escDetector = re.compile(b'(\textbackslash 033|~{)') *)
          self._mEscCharSetProber = None
          self._mCharSetProbers = []
          self.reset()
\end{lstlisting}

La búsqueda de todo el código para localizar otros usos del módulo \codigo{re} encuentra dos sitios más, ambos en \codigo{charsetprober.py}. De nuevo, el código define las expresiones regulares como cadenas de caracteres pero las está ejecutando sobre \codigo{aBuf}, que es un array de bytes. La solución es la misma: definir los patrones de la expresión regular como un array de bytes.


\begin{lstlisting}[escapeinside={(*}{*)}]
  class CharSetProber:
      .
      .
      .
      def filter_high_bit_only(self, aBuf):
(* \color{red}-         aBuf = re.sub(r'([\textbackslash x00-\textbackslash x7F])+', ' ', aBuf) *)
(* \color{green!50!black}+         aBuf = re.sub(b'([\textbackslash x00-\textbackslash x7F])+', b' ', aBuf) *)
          return aBuf
    
      def filter_without_english_letters(self, aBuf):
(* \color{red}-         aBuf = re.sub(r'([A-Za-z])+', ' ', aBuf) *)
(* \color{green!50!black}+         aBuf = re.sub(b'([A-Za-z])+', b' ', aBuf) *)
          return aBuf
\end{lstlisting}

\subsection{No puedo convertir un objeto 'bytes' en \codigo{str} implícitamente}

¡Curioso y requetecurioso!


\noindent\begin{minipage}{\textwidth}
\begin{lstlisting}[breaklines=true]
C:\home\chardet> python test.py tests\*\*
tests\ascii\howto.diveintomark.org.xml
Traceback (most recent call last):
  File "test.py", line 10, in <module>
    u.feed(line)
  File "C:\home\chardet\chardet\universaldetector.py", line 100, in feed
    elif (self._mInputState == ePureAscii) and self._escDetector.search(self._mLastChar + aBuf):
TypeError: Can't convert 'bytes' object to str implicitly
\end{lstlisting}
\end{minipage}

En este caso existe un desafortunado choque entre el estilo de codificación y el intérprete de Python. El \codigo{TypeError} podría estar en cualquier lugar de la línea, pero la traza no dice exactamente dónde está. Podría ser en la primera condición o en la segunda. Para acotarlo, deberías dividir la línea en dos, así:


\begin{lstlisting}[language=Python,breaklines=true]
elif (self._mInputState == ePureAscii) and \
    self._escDetector.search(self._mLastChar + aBuf):
\end{lstlisting}

Ejecutar de nuevo el test:


\noindent\begin{minipage}{\textwidth}
\begin{lstlisting}[breaklines=true]
C:\home\chardet> python test.py tests\*\*
tests\ascii\howto.diveintomark.org.xml
Traceback (most recent call last):
  File "test.py", line 10, in <module>
    u.feed(line)
  File "C:\home\chardet\chardet\universaldetector.py", line 101, in feed
    self._escDetector.search(self._mLastChar + aBuf):
TypeError: Can't convert 'bytes' object to str implicitly
\end{lstlisting}
\end{minipage}

¡Ajá! El problema no estaba  en la primera parte de la condición (\codigo{self.\_mInputState == ePureAscii}) sino en la segunda. ¿Qué ha podido causar el error? Quizás pienses que el método \codigo{search()} está esperando un valor de un tipo diferente, pero eso no generaría esta traza. Las funciones de Python pueden tomar cualquier valor; si pasas el número correcto de parámetros, la función se ejecutará. Puede \emph{cascar} si le pasas un valor de un tipo diferente el que esté esperando, pero si eso sucediera, la traza apuntaría a algún lugar interno de la función. Esta traza, sin embargo, dice que ni siquiera a podido llamar al método \codigo{search()}. Así que el problema debe estar en la operación \codigo{+}, cuando intenta construir el valor que posteriormente deberá pasarse a \codigo{search()}.

Sabemos por la depuración anterior que \codigo{aBuf} es un array de bytes. Pero ¿qué contiene \codigo{self.\_mLastChar}? Es una variable de instancia, definida en el método \codigo{reset()}, que se llama desde el método \codigo{\_\_init\_\_()}.


\begin{lstlisting}[language=Python,escapeinside={(*}{*)}]
class UniversalDetector:
    def __init__(self):
        self._highBitDetector = re.compile(b'[\x80-\xFF]')
        self._escDetector = re.compile(b'(\033|~{)')
        self._mEscCharSetProber = None
        self._mCharSetProbers = []
        (* \color{yellow!40!black} self.reset() *)

    def reset(self):
        self.result = {'encoding': None, 'confidence': 0.0}
        self.done = False
        self._mStart = True
        self._mGotData = False
        self._mInputState = ePureAscii
        (* \color{yellow!40!black} self.\_mLastChar = '' *)
\end{lstlisting}

Y ya tenemos nuestra respuesta. ¿La ves? \codigo{self.\_mLastChar} es una cadena de caracteres, pero \codigo{aBuf} es un array de bytes. Y no puedes concatenar una cadena de caracteres con un array de bytes ---ni siquiera con una cadena de caracteres vacía.

En cualquier caso ¿Qué es \codigo{self.\_mLastChar}? En el método \codigo{feed()}, mira unas líneas más abajo en el que la traza ha sucedido.


\begin{lstlisting}[language=Python,escapeinside={(*}{*)}]
if self._mInputState == ePureAscii:
    if self._highBitDetector.search(aBuf):
        self._mInputState = eHighbyte
    elif (self._mInputState == ePureAscii) and \
            self._escDetector.search(self._mLastChar + aBuf):
        self._mInputState = eEscAscii

        (* \color{yellow!40!black}self.\_mLastChar = aBuf[-1] *)
\end{lstlisting}

Se llama al método \codigo{feed()} una y otra vez con unos pocos bytes cada vez. El método procesa los bytes (pasados en \codigo{aBuf}), luego almacena el último byte en \codigo{self.\_mLastChar} por si se necesita en la siguiente llamada. (En una codificación multibyte el método \codigo{feed()} podría ser llamado a la mitad de un carácter para volver a ser llamado después con la otra mitad). Pero ya que \codigo{aBuf} ahora es un array de bytes en lugar de una cadena de caracteres, \codigo{self.\_mLastChar} también necesita ser un array de bytes. Así:

\begin{lstlisting}[language=Python,escapeinside={(*}{*)}]
  def reset(self):
      .
      .
      .
      (* \color{red}-     self.\_mLastChar = '' *)
      (* \color{green!50!black}+     self.\_mLastChar = b'' *)
\end{lstlisting}

Si buscamos a lo largo de todo el código por ``\codigo{mLastChar}'' encontramos un problema similar en \codigo{mbcharsetprober.py}, pero en lugar de mantener el último carácter, mantiene el recuerdo de los \emph{dos} últimos. La clase \codigo{MultiByteCharSetProber} utiliza una lista de cadenas de caracteres de un solo carácter para mantener los últimos dos caracteres. En Python 3 necesita utilizar una lista de enteros ya que en realidad no está manteniendo caracteres, sino bytes (Los bytes son enteros entre \codigo{0-255}).


\begin{lstlisting}[language=Python,escapeinside={(*}{*)}]
  class MultiByteCharSetProber(CharSetProber):
      def __init__(self):
          CharSetProber.__init__(self)
          self._mDistributionAnalyzer = None
          self._mCodingSM = None
          (* \color{red}-         self.\_mLastChar = ['\textbackslash x00', '\textbackslash x00'] *)
          (* \color{green!50!black}+         self.\_mLastChar = [0, 0] *)

      def reset(self):
          CharSetProber.reset(self)
          if self._mCodingSM:
              self._mCodingSM.reset()
          if self._mDistributionAnalyzer:
              self._mDistributionAnalyzer.reset()
          (* \color{red}-         self.\_mLastChar = ['\textbackslash x00', '\textbackslash x00'] *)
          (* \color{green!50!black}+         self.\_mLastChar = [0, 0] *)
\end{lstlisting}

\subsection{Tipo del operando no soportado para +: 'int' y 'bytes'}

Tengo buenas y malas noticias. Las buenas noticias es que estamos haciendo progresos\ldots


\noindent\begin{minipage}{\textwidth}
\begin{lstlisting}[breaklines=true]
C:\home\chardet> python test.py tests\*\*
tests\ascii\howto.diveintomark.org.xml
Traceback (most recent call last):
  File "test.py", line 10, in <module>
    u.feed(line)
  File "C:\home\chardet\chardet\universaldetector.py", line 101, in feed
    self._escDetector.search(self._mLastChar + aBuf):
TypeError: unsupported operand type(s) for +: 'int' and 'bytes'
\end{lstlisting}
\end{minipage}

\ldots las malas noticias es que no siempre da esa sensación.

¡Pero es progreso! ¡En serio! Incluso aunque la traza da el error en la misma línea de código: es un error diferente al que era. ¡Progreso! ¿Cuál es el problema ahora? La última vez miramos, esta línea de código no intentaba concatenar un \codigo{int} con un array de bytes(\codigo{bytes}). De hecho, hemos pasado un buen rato tratando de asegurar que \codigo{self.\_mLastChar} fuera un array de bytes. Cómo se convirtió en un \codigo{int}.

La respuesta descansa en las líneas siguientes de código:

\begin{lstlisting}[language=Python,escapeinside={(*}{*)}]
if self._mInputState == ePureAscii:
    if self._highBitDetector.search(aBuf):
        self._mInputState = eHighbyte
    elif (self._mInputState == ePureAscii) and \
            self._escDetector.search(self._mLastChar + aBuf):
        self._mInputState = eEscAscii

        (* \color{yellow!40!black}self.\_mLastChar = aBuf[-1] *)
\end{lstlisting}

\cajaTexto{Cada elemento en una cadena de caracteres es una cadena de caracteres. Cada elemento en una array de bytes es un entero}

Este error no sucede la primera vez que se ejecuta el método \codigo{feed()}. Ocurre la \emph{segunda vez}, después de que \codigo{self.\_mLastChar} se haya activado con el último byte de \codigo{aBuf}. Bien ¿Cuál es el problema? Al obtener un elemento de un array de bytes se genera un valor entero (\codigo{int}), no un array de bytes. Para ver la diferencia, sígueme en esta consola interactiva:

\begin{lstlisting}[breaklines=true]
>>> aBuf = b'\xEF\xBB\xBF'
>>> len(aBuf)
3
>>> mLastChar = aBuf[-1]
>>> mLastChar            
191
>>> type(mLastChar)      
<class 'int'>
>>> mLastChar + aBuf    
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: unsupported operand type(s) for +: 'int' and 'bytes'
>>> mLastChar = aBuf[-1:]
>>> mLastChar
b'\xbf'
>>> mLastChar + aBuf    
b'\xbf\xef\xbb\xbf'
\end{lstlisting}

\begin{enumerate}
  \item \emph{Línea 1:} Define un array de bytes de longitud 3.
  \item \emph{Línea 5:} El último elemento del array de bytes es 191.
  \item \emph{Línea 7:} Es un entero.
  \item \emph{Línea 9:} No se puede concatenar un entero con un array de bytes. Con esto está replicado el error que se ha encontrado en \codigo{universaldetector.py}.
  \item \emph{Línea 13:} Esta es la forma de arreglarlo. En lugar de tomar el último elemento del array de bytes, utiliza la selección de listas para crear un nuevo array de bytes de un único elemento. Este array de bytes comienza con el último elemento y continua la selección hasta el final del array. Ahora \codigo{mLastChar} contiene un array de longitud 1.
  \item \emph{Línea 16:} Al concatenar un array de longitud 1 con otro de longitud 3 da como resultado un nuevo array de bytes de longitud 4.
\end{enumerate}

Así, para asegurar que el método \codigo{feed()} de \codigo{universaldetector.py} continúa funcionando todas las veces que sea llamado, necesitas inicializar \codigo{self.\_mLastChar} como un array de bytes de longitud cero, y luego asegurar que sigue conteniendo un array de bytes permanentemente.


\begin{lstlisting}[language=Python,escapeinside={(*}{*)}]
              self._escDetector.search(self._mLastChar + aBuf):
          self._mInputState = eEscAscii

(* \color{red}- self.\_mLastChar = aBuf[-1] *)
(* \color{green!50!black}+ self.\_mLastChar = aBuf[-1:] *)
\end{lstlisting}

\subsection{\codigo{ord()} esperaba una cadena de caracteres de longitud 1, pero encontró un \codigo{int}}

¿Cansado ya? Casi hemos terminado\ldots


\noindent\begin{minipage}{\textwidth}
\begin{lstlisting}[breaklines=true]
C:\home\chardet> python test.py tests\*\*
tests\ascii\howto.diveintomark.org.xml   ascii with confidence 1.0
tests\Big5\0804.blogspot.com.xml
Traceback (most recent call last):
  File "test.py", line 10, in <module>
    u.feed(line)
  File "C:\home\chardet\chardet\universaldetector.py", line 116, in feed
    if prober.feed(aBuf) == constants.eFoundIt:
  File "C:\home\chardet\chardet\charsetgroupprober.py", line 60, in feed
    st = prober.feed(aBuf)
  File "C:\home\chardet\chardet\utf8prober.py", line 53, in feed
    codingState = self._mCodingSM.next_state(c)
  File "C:\home\chardet\chardet\codingstatemachine.py", line 43, in next_state
    byteCls = self._mModel['classTable'][ord(c)]
TypeError: ord() expected string of length 1, but int found
\end{lstlisting}
\end{minipage}

Ok, así que \codigo{c} contiene un número entero, pero la función \codigo{ord()} estaba esperando una cadena de caracteres de un solo carácter. Es justo. ¿Dónde está definido \codigo{c}?

\begin{lstlisting}[language=Python,escapeinside={(*}{*)}]
# codingstatemachine.py
def next_state(self, c):
    # for each byte we get its class
    # if it is first byte, we also get byte length
    byteCls = self._mModel['classTable'][ord(c)]
\end{lstlisting}

Este no es de ayuda; simplemente sabemos que es un parámetro de la función. Vamos a tirar de la traza.

\begin{lstlisting}[language=Python,escapeinside={(*}{*)}]
# utf8prober.py
def feed(self, aBuf):
    for c in aBuf:
        codingState = self._mCodingSM.next_state(c)
\end{lstlisting}

¿Lo ves? En Python 2 \codigo{aBuf} era una cadena de caracteres, así que \codigo{c} era una cadena de un solo carácter (Eso es lo que recuperas cuando iteras a través de una cadena de caracteres ---todos los caracteres, de uno en uno). Pero ahora \codigo{aBuf} es un array de bytes, así que \codigo{c} es un \codigo{int}, no una cadena de un carácter. En otras palabras, no hay necesidad de llamar a la función \codigo{ord()} ya que \codigo{c} ya es de tipo entero.

Así que:


\begin{lstlisting}[language=Python,escapeinside={(*}{*)}]
  def next_state(self, c):
      # for each byte we get its class
      # if it is first byte, we also get byte length
      (* \color{red}-     byteCls = self.\_mModel['classTable'][ord(c)] *)
      (* \color{green!50!black}+     byteCls = self.\_mModel['classTable'][c] *)
\end{lstlisting}

Si se busca en el código por llamadas a \codigo{ord(c)} se descubren problemas similares en \codigo{sbcharsetprober.py}\ldots

\begin{lstlisting}[language=Python,escapeinside={(*}{*)}]
# sbcharsetprober.py
def feed(self, aBuf):
    if not self._mModel['keepEnglishLetter']:
        aBuf = self.filter_without_english_letters(aBuf)
    aLen = len(aBuf)
    if not aLen:
        return self.get_state()
    for c in aBuf:
    (* \color{yellow!40!black} order = self.\_mModel['charToOrderMap'][ord(c)] *)
\end{lstlisting}

Y en \codigo{latin1prober.py}\ldots


\begin{lstlisting}[language=Python,escapeinside={(*}{*)}]
# latin1prober.py
def feed(self, aBuf):
    aBuf = self.filter_with_english_letters(aBuf)
    for c in aBuf:
    (* \color{yellow!40!black}charClass = Latin1\_CharToClass[ord(c)] *)
\end{lstlisting}

\codigo{c} itera sobre \codigo{aBuf}, lo que significa que es un entero, no una cadena de caracteres de longitud 1. La solución es la misma: cambiar \codigo{ord(c)} por \codigo{c}.


\begin{lstlisting}[language=Python,escapeinside={(*}{*)}]
  # sbcharsetprober.py
  def feed(self, aBuf):
      if not self._mModel['keepEnglishLetter']:
          aBuf = self.filter_without_english_letters(aBuf)
      aLen = len(aBuf)
      if not aLen:
          return self.get_state()
      for c in aBuf:
          (* \color{red}-         order = self.\_mModel['charToOrderMap'][ord(c)] *)
          (* \color{green!50!black}+         order = self.\_mModel['charToOrderMap'][c] *)

  # latin1prober.py
  def feed(self, aBuf):
      aBuf = self.filter_with_english_letters(aBuf)
      for c in aBuf:
          (*\color{red}-         charClass = Latin1\_CharToClass[ord(c)] *)
          (*\color{green!50!black}+         charClass = Latin1\_CharToClass[c] *)
\end{lstlisting}

\subsection{Tipos no ordenables: \codigo{int() >= str()}}

Sigamos:

\noindent\begin{minipage}{\textwidth}
\begin{lstlisting}[breaklines=true]
C:\home\chardet> python test.py tests\*\*
tests\ascii\howto.diveintomark.org.xml                       ascii with confidence 1.0
tests\Big5\0804.blogspot.com.xml
Traceback (most recent call last):
  File "test.py", line 10, in <module>
    u.feed(line)
  File "C:\home\chardet\chardet\universaldetector.py", line 116, in feed
    if prober.feed(aBuf) == constants.eFoundIt:
  File "C:\home\chardet\chardet\charsetgroupprober.py", line 60, in feed
    st = prober.feed(aBuf)
  File "C:\home\chardet\chardet\sjisprober.py", line 68, in feed
    self._mContextAnalyzer.feed(self._mLastChar[2 - charLen :], charLen)
  File "C:\home\chardet\chardet\jpcntx.py", line 145, in feed
    order, charLen = self.get_order(aBuf[i:i+2])
  File "C:\home\chardet\chardet\jpcntx.py", line 176, in get_order
    if ((aStr[0] >= '\x81') and (aStr[0] <= '\x9F')) or \
TypeError: unorderable types: int() >= str()
\end{lstlisting}
\end{minipage}

¿Qué significa esto? ¿Tipos no ordenables? De nuevo, la diferencia entre los arrays de bytes y las cadenas de caracteres nos ataca. Echa un vistazo al código:


\begin{lstlisting}[language=Python,escapeinside={(*}{*)}]
class SJISContextAnalysis(JapaneseContextAnalysis):
    def get_order(self, aStr):
        if not aStr: return -1, 1
        # find out current char's byte length
        (* \color{yellow!40!black}if ((aStr[0] >= '\textbackslash x81') and (aStr[0] <= '\textbackslash x9F')) or \textbackslash *)
           ((aStr[0] >= '\xE0') and (aStr[0] <= '\xFC')):
            charLen = 2
        else:
            charLen = 1
\end{lstlisting}

¿Y de dónde viene \codigo{aStr}? Miremos la traza:


\begin{lstlisting}[language=Python,escapeinside={(*}{*)}]
def feed(self, aBuf, aLen):
    .
    .
    .
    i = self._mNeedToSkipCharNum
    while i < aLen:
        (* \color{yellow!40!black}order, charLen = self.get\_order(aBuf[i:i+2]) *)
\end{lstlisting}

Otra vez nuestro viejo amigo \codigo{aBuf}. Como ya has adivinado de todos los problemas anteriores \codigo{aBuf} es un array de bytes. Aquí, el método \codigo{feed()} no lo pasa completo; pasa una sección. Pero como viste anteriormente en el capítulo, una partición de un array de bytes devuelve un array de bytes, así que el parámetro \codigo{aStr} que se pasa, a su vez, al método \codigo{get\_order()} es un array de bytes.

¿Y qué intenta hacer el código con \codigo{aStr}? Toma el primer elemento del array de bytes y lo compara con una cadena de longitud 1. En Python 2 esto funcionaba porque \codigo{aStr} y \codigo{aBuf} eran cadenas, y \codigo{aStr[0]} seguía siendo una cadena de caracteres, y se pueden comparar cadenas de carateres para ver si son distintas. Pero en Python 3 \codigo{aStr} y \codigo{aBuf} son arrays de bytes, \codigo{aStr[0]} es un número entero y no se pueden comparar enteros y cadenas de caracteres sin realizar la conversión de uno de ellos.

En este caso no hay necesidad de hacer el código más complicado añadiendo una conversión explícita. \codigo{aStr[0]} devuelve un entero; las cosas que estás comparanto son constantes. Así que vamos a cambiarlas de ser cadenas de caracteres de longitud 1 a números enteros. Y mientras estamos en ello, vamos a cambiar \codigo{aStr} a \codigo{aBuf}, ya que realmente no es una cadena de caracteres.

\begin{lstlisting}[language=Python,escapeinside={(*}{*)}]
  class SJISContextAnalysis(JapaneseContextAnalysis):
(* \color{red}-     def get\_order(self, aStr): *)
(* \color{red}-      if not aStr: return -1, 1 *)
(* \color{green!50!black}+     def get\_order(self, aBuf): *)
(* \color{green!50!black}+      if not aBuf: return -1, 1 *)
          # find out current char's byte length
(* \color{red}-         if ((aStr[0] >= '\textbackslash x81') and (aStr[0] <= '\textbackslash x9F')) or \ *)
(* \color{red}-            ((aBuf[0] >= '\textbackslash xE0') and (aBuf[0] <= '\textbackslash xFC')): *)
(* \color{green!50!black}+         if ((aBuf[0] >= 0x81) and (aBuf[0] <= 0x9F)) or \textbackslash  *)
(* \color{green!50!black}+            ((aBuf[0] >= 0xE0) and (aBuf[0] <= 0xFC)): *)
              charLen = 2
          else:
              charLen = 1

          # return its order if it is hiragana
(* \color{red}-      if len(aStr) > 1: *)
(* \color{red}-             if (aStr[0] == '\textbackslash 202') and \ *)
(* \color{red}-                (aStr[1] >= '\textbackslash x9F') and \ *)
(* \color{red}-                (aStr[1] <= '\textbackslash xF1'): *)
(* \color{red}-                return ord(aStr[1]) - 0x9F, charLen *)
(* \color{green!50!black}+      if len(aBuf) > 1: *)
(* \color{green!50!black}+             if (aBuf[0] == 202) and \ *)
(* \color{green!50!black}+                (aBuf[1] >= 0x9F) and \ *)
(* \color{green!50!black}+                (aBuf[1] <= 0xF1): *)
(* \color{green!50!black}+                return aBuf[1] - 0x9F, charLen *)

          return -1, charLen

  class EUCJPContextAnalysis(JapaneseContextAnalysis):
(* \color{red}-     def get\_order(self, aStr): *)
(* \color{red}-      if not aStr: return -1, 1 *)
(* \color{green!50!black}+     def get\_order(self, aBuf): *)
(* \color{green!50!black}+      if not aBuf: return -1, 1 *)
          # find out current char's byte length
(* \color{red}-         if (aStr[0] == '\textbackslash x8E') or \ *)
(* \color{red}-           ((aStr[0] >= '\textbackslash xA1') and (aStr[0] <= '\textbackslash xFE')): *)
(* \color{green!50!black}+         if (aBuf[0] == 0x8E) or \ *)
(* \color{green!50!black}+           ((aBuf[0] >= 0xA1) and (aBuf[0] <= 0xFE)): *)
              charLen = 2
(* \color{red}-         elif aStr[0] == '\textbackslash x8F': *)
(* \color{green!50!black}+         elif aBuf[0] == 0x8F: *)
              charLen = 3
          else:
              charLen = 1

        # return its order if it is hiragana
(* \color{red}-    if len(aStr) > 1: *)
(* \color{red}-           if (aStr[0] == '\textbackslash xA4') and \ *)
(* \color{red}-              (aStr[1] >= '\textbackslash xA1') and \ *)
(* \color{red}-              (aStr[1] <= '\textbackslash xF3'): *)
(* \color{red}-                 return ord(aStr[1]) - 0xA1, charLen *)
(* \color{green!50!black}+    if len(aBuf) > 1: *)
(* \color{green!50!black}+           if (aBuf[0] == 0xA4) and \ *)
(* \color{green!50!black}+              (aBuf[1] >= 0xA1) and \ *)
(* \color{green!50!black}+              (aBuf[1] <= 0xF3): *)
(* \color{green!50!black}+               return aBuf[1] - 0xA1, charLen *)

        return -1, charLen
\end{lstlisting}

La búsqueda de más ocurrencias de la función \codigo{ord()} en el código descubre el mismo problema en \codigo{chardistribution.py} (específicamente en \codigo{EUCTWDistributionAnalysis}, \codigo{EUCKRDistributionAnalysis}, \codigo{GB2312DistributionAnalysis}, \codigo{Big5DistributionAnlysis}, \codigo{SJIDistributionAnalysis}, y \codigo{EUCJPDistributionAnalysis}. En cada caso, la subsanación es similar a la realizada en las clases \codigo{EUCJPContextAnalysis} y \codigo{SJISContextAnalysis} de \codigo{jpcntx.py}.

\subsection{El nombre global ``\codigo{reduce}'' no está definido}

De nuevo en la brecha\ldots


\noindent\begin{minipage}{\textwidth}
\begin{lstlisting}[breaklines=true]
C:\home\chardet> python test.py tests\*\*
tests\ascii\howto.diveintomark.org.xml                       ascii with confidence 1.0
tests\Big5\0804.blogspot.com.xml
Traceback (most recent call last):
  File "test.py", line 12, in <module>
    u.close()
  File "C:\home\chardet\chardet\universaldetector.py", line 141, in close
    proberConfidence = prober.get_confidence()
  File "C:\home\chardet\chardet\latin1prober.py", line 126, in get_confidence
    total = reduce(operator.add, self._mFreqCounter)
NameError: global name 'reduce' is not defined
\end{lstlisting}
\end{minipage}

De acuerdo a la guía oficial ``Qué es lo nuevo de Python 3.0''\footnote{\href{http://docs.python.org/3.0/whatsnew/3.0.html\#builtins}{http://docs.python.org/3.0/whatsnew/3.0.html\#builtins}}, la función \codigo{reduce()} se ha movido del espacio global de nombres al módulo \codigo{functools}. Parafraseando a la guía: utiliza \codigo{functools.reduce()} si realmente lo necesitas; sin embargo, el 99\% de las ocasiones, un buble \codigo{for loop} resulta más legible. Puedes leer más sobre esta decisión de Guido van Rossum en su blog: La fortuna de reduce() en Python 3.00\footnote{\href{http://www.artima.com/weblogs/viewpost.jsp?thread=98196}{http://www.artima.com/weblogs/viewpost.jsp?thread=98196}}.


\begin{lstlisting}[language=Python,escapeinside={(*}{*)}]
def get_confidence(self):
    if self.get_state() == constants.eNotMe:
        return 0.01
  
        (* \color{yellow!40!black}total = reduce(operator.add, self.\_mFreqCounter) *)
\end{lstlisting}

La función \codigo{reduce()} toma dos parámetros ---una función y una lista (estrictamente, un objeto iterable)--- y aplica la función acumulativamente a cada elemento de la lista. En otras palabras, es una forma de sumar todos los elementos de una lista y devolver el resultado.

Esta monstruosidad era tan común que Python añadió una función \codigo{sum()} global.


\begin{lstlisting}[language=Python,escapeinside={(*}{*)}]
def get_confidence(self):
      if self.get_state() == constants.eNotMe:
          return 0.01

          (* \color{red}-     total = reduce(operator.add, self.\_mFreqCounter) *)
          (* \color{green!50!black}+     total = sum(self.\_mFreqCounter) *)
\end{lstlisting}

Ya que dejamos de usar el módulo \codigo{operator}, podemos suprimir el \codigo{import} correspondiente del fichero:


\begin{lstlisting}[language=Python,escapeinside={(*}{*)}]
from .charsetprober import CharSetProber
  from . import constants
  (* \color{red}- import operator *)
\end{lstlisting}

¿Otra prueba?

\begin{lstlisting}[breaklines=true]
C:\home\chardet> python test.py tests\*\*
tests\ascii\howto.diveintomark.org.xml                       ascii with confidence 1.0
tests\Big5\0804.blogspot.com.xml                             Big5 with confidence 0.99
tests\Big5\blog.worren.net.xml                               Big5 with confidence 0.99
tests\Big5\carbonxiv.blogspot.com.xml                        Big5 with confidence 0.99
tests\Big5\catshadow.blogspot.com.xml                        Big5 with confidence 0.99
tests\Big5\coolloud.org.tw.xml                               Big5 with confidence 0.99
tests\Big5\digitalwall.com.xml                               Big5 with confidence 0.99
tests\Big5\ebao.us.xml                                       Big5 with confidence 0.99
tests\Big5\fudesign.blogspot.com.xml                         Big5 with confidence 0.99
tests\Big5\kafkatseng.blogspot.com.xml                       Big5 with confidence 0.99
tests\Big5\ke207.blogspot.com.xml                            Big5 with confidence 0.99
tests\Big5\leavesth.blogspot.com.xml                         Big5 with confidence 0.99
tests\Big5\letterlego.blogspot.com.xml                       Big5 with confidence 0.99
tests\Big5\linyijen.blogspot.com.xml                         Big5 with confidence 0.99
tests\Big5\marilynwu.blogspot.com.xml                        Big5 with confidence 0.99
tests\Big5\myblog.pchome.com.tw.xml                          Big5 with confidence 0.99
tests\Big5\oui-design.com.xml                                Big5 with confidence 0.99
tests\Big5\sanwenji.blogspot.com.xml                         Big5 with confidence 0.99
tests\Big5\sinica.edu.tw.xml                                 Big5 with confidence 0.99
tests\Big5\sylvia1976.blogspot.com.xml                       Big5 with confidence 0.99
tests\Big5\tlkkuo.blogspot.com.xml                           Big5 with confidence 0.99
tests\Big5\tw.blog.xubg.com.xml                              Big5 with confidence 0.99
tests\Big5\unoriginalblog.com.xml                            Big5 with confidence 0.99
tests\Big5\upsaid.com.xml                                    Big5 with confidence 0.99
tests\Big5\willythecop.blogspot.com.xml                      Big5 with confidence 0.99
tests\Big5\ytc.blogspot.com.xml                              Big5 with confidence 0.99
tests\EUC-JP\aivy.co.jp.xml                                  EUC-JP with confidence 0.99
tests\EUC-JP\akaname.main.jp.xml                             EUC-JP with confidence 0.99
tests\EUC-JP\arclamp.jp.xml                                  EUC-JP with confidence 0.99
.
.
.
316 tests
\end{lstlisting}

¡Funciona! ¡Quiero bailar un ratito!\footnote{\href{http://www.hampsterdance.com/}{http://www.hampsterdance.com/}}

\section{Resumen}

¿Qué hemos aprendido?

\begin{enumerate}
  \item Migrar una cantidad no trivial de código de Python 2 a Python 3 es un ``dolor''. No hay forma de evitarlo. Es duro.
  \item La herramienta \codigo{2to3} es útil pero solamente cubre lo más fácil: cambios de nombre de funciones, cambios de nombre de módulos, cambios de sintaxis. Es una pieza de ingeniería impresionante, pero al final es solo un robot de búsqueda y sustitución avanzada.
   \item El primer problema en esta librería fue la diferencia existente entre cadenas de caracteres y bytes. En este caso parece obvio ya que el objetivo de esta librería es convertir bytes en cadenas de caracteres. Pero esto sucede más a menudo de lo que parece. Cualquier lectura de un fichero en modo binario devuelve un array de bytes. Cuando se recupera una página web, cuando se llama a una API en la web. Todo ello devuelve un array de bytes.

    \item Necesitas conocer tu programa. Profundamente. Muchas veces lo has escrito tú, pero otras no. Hay que conocer el porqué de su código. Los errores aparecen en cualquier sitio y hay que corregirlos.
    \item Los casos de prueba son fundamentales. No migres nada sin ellos. La única razón por la que tengo confianza en que \codigo{chardet} funcione en Python 3 es que comencé con una suite de pruebas que recorre la mayor parte del código de esta librería. Si no dispones de pruebas, escribe algunas antes de iniciar la migración a Python 3. Si tienes algunas pruebas: escribe más. Si tienes muchas pruebas, puedes empezar la diversión.

\end{enumerate}
